#import "template.typ": *
#import "@preview/wordometer:0.1.4": word-count, total-characters
#import "@preview/lovelace:0.3.0": *
#import "generated.typ": *

#set raw(syntaxes: "nunchaku.sublime-syntax")
#show math.equation: it => {
  show ".": [$.thin$]
  it
}

#show figure.where(
  kind: table
): set figure.caption(position: top)

#show figure.where(
  kind: table
): set figure(placement: top)

#let inductive = $bold("inductive")$
#let definition = $bold("def")$
#let Type = $"Type"$
#let Prop = $"Prop"$
#let type = $"type"$
#let prop = $"prop"$
#let where = $bold("where")$
#let pred = $bold("pred")$

#let target_date = datetime(year: 2026, month: 1, day: 23)
#show : official.with(
  title: [
    Finite Model Finding for Lean 4
  ],
  author: "Henrik Böving",
  email: "H.Boeving@campus.lmu.de",
  matriculation: [TODO],
  thesis-type: [Master's Thesis],
  advisor: [Prof. Dr. Jasmin Blanchette],
  supervisor: [Prof. Dr. Jasmin Blanchette],
  submission_date: target_date.display("[month repr:long] [day], [year]"),
  glossary: (
    (key: "DTT", short: "DTT", long: "dependent type theory"),
    (key: "HOL", short: "HOL", long: "higher-order logic"),
    (key: "ITP", short: "ITP", long: "interactive theorem proving"),
  ),
  abstract: [
    When conducting formalization projects, a significant time sink can be attempts to prove
    theorems that are not actually correct. This can, for example, occur due to subtle mistakes in
    definitions or forgetting to state an assumption. Counterexample-finding techniques can be used
    to reduce this time sink by identifying incorrect theorems before time is spent attempting
    to prove them.

    This thesis describes work on a new counterexample-finding tool for the Lean 4 proof assistant.
    Unlike previous approaches for counterexample-finding in dependent type theories, it relies on
    finite model finding. For this I identify a practically significant fragment of Lean's logic
    and develop a reduction from it to the Nunchaku finite model finder. In addition, I develop
    extensions to Nunchaku that improve its performance on the kinds of problems generated by the
    reduction.
  ],
  acknowledgement: [
    I would like to thank Jasmin Blanchette for his supervision and, in particular, sharing details
    about Nitpick that helped improve Nunchaku during this work. Furthermore I would like to thank
    Arthur Adjedj, Joachim Breitner, Siddharth Bhat, and Simon Cruanes for many fruitful discussion
    on both the theory and implementation side of this work. Finally, I would like to thank
    Abdalrhman Mohamed and Andrew Reynolds for implementing new features and fixing bugs in cvc5
    that were crucial for this work.
  ]
)
#show: word-count
#show figure.caption : set text(10pt)

= Introduction <sect_intro>

The growing pervasiveness and sophistication of modern computing systems has increased both their
susceptibility to faults and the scale of damage such faults can cause. Ensuring that critical
mistakes in software are identified before they lead to serious consequences is therefore a vital challenge.
An increasingly popular approach to tackling this challenge is the use of @ITP systems such as
Isabelle @isabelle, Lean @lean4, and Rocq @rocq. However, developing correctness proofs
in these systems at the same time as developing definitions and theorem statements can turn out to
be quite frustrating. Often times definitions turn out to be subtly wrong, or hypotheses in theorems
need to be strengthened to make them provable. This problem can be partially mitigated through the
use of counterexample-finding techniques. These techniques can help users identify such issues early,
before significant time is wasted on unprovable goals.

For this reason, a variety of counterexample-finding techniques have been incorporated into @ITP
systems over the years. For example, Isabelle provides support for random testing,
bounded exhaustive testing, narrowing, and finite model finding, implemented in
Nitpick @nitpick and Quickcheck @quickcheck. In contrast, systems grounded in @DTT have
predominantly relied on random testing with tools like Plausible @plausible and QuickChick @quickchick.
These tools attempt to synthesize a procedure that evaluates the theorem
statement for specific input values. This procedure is then executed repeatedly with random inputs
to search for counterexamples. While this technique is quite effective in general, it does have a
crucial limitation: If the inputs to the theorems are constrained by complicated invariants, it can
be quite difficult to generate candidates for evaluation that satisfy these preconditions.
For such theorems, finite model finders, such as Nitpick, shine. Nitpick reduces the original theorem to
the boolean satisfiability (SAT) problem and invokes a SAT solver to search for counterexamples.
The SAT solver can then search for inputs with invariants more directly, rather than relying on
random generation to eventually find them.

In this thesis I describe the first finite model finding approach that is integrated with a dependently
typed theorem prover, namely Lean. The core contribution is the reduction of a practically relevant
fragment of Lean to the input logic of Nitpick's spiritual successor, Nunchaku @nunchakudtt.
First I describe the general idea behind the reduction in @sect_reduct.
Then I discuss the design of the new Chako counterexample finder based on this reduction
in @sect_impl. Lastly, I present an evaluation of Chako on a few case studies in
@sect_case_studies and parts of the Lean standard library in @sect_eval.

#pagebreak(weak: true)

= Related Work <sect_related>
Many counterexample-finding techniques have been integrated with @ITP systems in the past. These techniques
can generally be located somewhere on a spectrum between concrete execution and fully symbolic
reasoning.

On the concrete end of the spectrum, the most widely deployed technique is the already mentioned
random testing. Popularized by Haskell's QuickCheck @haskellquickcheck, random testing has been
implemented in many @ITP systems, such as ACL2 @acl2cex, Agda's QuickCheck @agdaquickcheck,
Isabelle's Quickcheck @quickcheck, Lean's Plausible @plausible, PVS @pvsquickcheck, and Rocq's QuickChick @quickchick.
The main strength of this approach is performance: computers can generate and
evaluate many candidates within a short time span. As previously explained, the main weakness
are complex constraints on the input space that make it hard to generate valid inputs.
In order to counteract this, ACL2, Isabelle's Quickcheck, and later iterations of Rocq's QuickChick @quickchick-multirelations
implement techniques to derive generators for inputs that are constrained by certain kinds of
inductive relations.

Another concrete execution technique is bounded exhaustive testing, as implemented in
Isabelle's Quickcheck and as a QuickChick extension @boundex-coq. Bounded exhaustive testing
generates and tests all possible inputs to a conjecture up to a certain bound (e.g. term size).
Unlike random testing, this ensures that all potentially relevant inputs within the search
space are actually tested. The main limitation is that the search space can often only be fully
explored up to a small bound within a reasonable time frame. This causes bounded exhaustive testing to
usually miss larger counterexamples.

A fundamental limitation of systems that rely purely on concrete execution is the inability to
refute propositions that are not executable. For example, to demonstrate that
$forall n : NN. exists m : NN. n + m = m$ is false using concrete execution, the system would
need to generate a value for $n$ and then try all possible values for $m$. This is impossible,
given that $m$ ranges over infinitely many values. Thus, to find these kinds of counterexamples,
symbolic reasoning is necessary.

Between the fully symbolic and fully concrete ends of the spectrum, Isabelle's Quickcheck also
implements a narrowing-based approach. The idea of narrowing is to symbolically evaluate the
proposition with partially instantiated terms first and then refine them until a counterexample can be found.

On the far symbolic end of the spectrum, we can find techniques that perform reductions to SAT or SMT
solvers. These techniques can roughly be separated into finite model finding and counterexample-producing
decision procedures. The latter often occurs as a by-product of integrating SAT and SMT solvers with @ITP
systems, as done in Isabelle's `smt` @isabellesmt, Lean's lean-smt @leansmt, and Rocq's SMTCoq @smtcoq.
Finite model finding techniques enumerate all potential finite models of a conjecture in search of
a false model. This procedure is usually powered by a reduction to SAT or SMT.
Finite model finding has, so far, only been deployed to Isabelle in the form of Nitpick @nitpick and Refute @refute.

#pagebreak(weak: true)

= Background <sect_background>
In this section I provide an overview of the fundamentals of the source and target languages involved in
the reduction, focusing on the fragments relevant to this work. Additional details are discussed
later in @sect_reduct and @sect_impl.

== Lean 4 <sect_lean4>
Lean 4 @lean4 is an open-source interactive theorem prover and functional programming language.
Originally developed by Leonardo de Moura at Microsoft Research and Sebastian Ullrich at KIT, it is
nowadays maintained by the Lean FRO with many open-source contributions by other volunteers. On the theorem-proving side,
Lean's logic is a dependent type theory based on the calculus of inductive constructions @coc @cic.
This base calculus is extended with several axioms, most notably quotients, function extensionality, and
choice. A thorough theoretical description of Lean's logic and its properties can be found
in the works of Carneiro @mario-type-theory and Ullrich @sebastianphd.

The syntax of Lean's core expression language is given by the grammar
$
  e ::= x | c | e " " e | lambda x : e . e | (x : e) -> e | "Sort" u
$
Just like the simply typed lambda calculus, Lean supports variables, constants, function application,
and function abstraction. Unlike the simply typed lambda calculus, it does not have a function
type $A -> B$ but instead the dependent function type $(x : A) -> B$. The crucial
difference being that the _term_ variable $x$ may occur in the _type_ $B$, we say $B$ may depend on
$x$. For example, we can denote the type of functions that take a natural number $n$ and return a number
less than $n$ as $(n : NN) -> "Fin" n$. If $B$ does not depend on $x$, the normal function type
notation $A ->B$ can be used instead.

Lean supports several kinds of constants, with the most prominent ones being definitions, theorems,
and inductive types. Definitions and theorems consist of a name, type or statement, and a body.
The body may refer to the definition recursively in the surface-level syntax.
While Lean internally desugars these recursive definitions into a non-recursive
representation, we will work with the recursive equations that get automatically
derived from the original syntax. Inductive types are the primary mechanism to introduce new types
in Lean. They are defined by listing their constructors, which specify the ways in which values of
the type can be built. A few examples of basic inductive types and definitions are given in @basic-inductives.

As alluded to in @basic-inductives by the `: Type` notation, these basic inductive types also have a type:
$Type$. In fact, Lean supports a whole hierarchy of types $Type : Type 1 : ... : Type u : Type (u + 1)$
in order to avoid type-theoretic versions of Russell's paradox. At the bottom of
this hierarchy sits the type of mathematical propositions $Prop : Type$. Both of these concepts
are unified with the expression $"Sort" u$ by defining $Prop := "Sort" 0$ and $Type u := "Sort" (u + 1)$.

#figure(
  ```lean
  inductive Unit : Type where
    | unit

  inductive Bool : Type where
    | true
    | false

  def Bool.not (b : Bool) : Bool :=
    match b with
    | true => Bool.false
    | false => Bool.true

  -- types may be recursive
  inductive Nat : Type where
    | zero
    | succ (n : Nat)

  -- types may contain proofs of invariants
  inductive Fin (n : Nat) : Type where
    | mk (val : Nat) (h : val < n)

  -- types may be generic over other types
  inductive Prod (α : Type) (β : Type) : Type where
    | mk (fst : α) (snd : α)

  def Prod.fst {α : Type} {β : Type} (p : Prod α β) : α :=
    match p with
    | .mk fst _ => fst

  -- dependent product type
  inductive Sigma (α : Type) (β : α → Type) : Type where
    | mk (fst : α) (snd : β fst)

  inductive List (α : Type) : Type where
    | nil
    | cons (x : α) (xs : List α)

  -- definitions may be recursive
  def List.length {α : Type} (xs : List α) : Nat :=
    match xs with
    | nil => Nat.zero
    | cons _ ys => Nat.succ (List.length ys)

  -- lists of length n
  inductive Vec (α : Type) : Nat → Type where
    | nil : Vec α 0
    | cons {n : Nat} (x : α) (xs : Vec α n) : Vec α (n + 1)
  ```,
  caption: "Basic Inductive Types and Definitions",
  gap: 1.5em,
) <basic-inductives>

#pagebreak(weak: true)

Proofs in Lean are done using the Curry-Howard correspondence: Given a type $P : Prop$, if we
can construct an inhabitant of that type $h : P$, then $h$ is a proof of $P$. The main
motivation for introducing $Prop$ as a separate concept is to allow for impredicativity and proof
irrelevance. That is, we have $(forall x : A. P) : Prop$ and additionally for any two inhabitants
$h_1, h_2 : P$ we get $h_1 = h_2$ by definition. Proof irrelevance is going to be of particular
interest later on because it ensures that proof terms cannot be computationally relevant.
This allows us to always erase proofs without changing the semantics of a Lean program.

On top of these base concepts, Lean provides a myriad of additional concepts that all get reduced to
the core calculus through a process called elaboration. These reductions are performed by programs
called elaborators that are also written in Lean. The mechanism for integrating additional elaborators is so powerful
that completely new languages have been developed entirely as extensions to Lean itself. For
example, Veil @veil implements a language for defining and reasoning about distributed systems within
Lean.

On the type level, a very common extension is `structure`. A structure is an inductive type with only one
constructor. For each structure, Lean automatically derives projection functions to obtain each of the
parameters of that constructor. Using structures, the `Prod` definition from @basic-inductives could
have also been written as
```lean
structure Prod (α : Type) (β : Type) where
  fst : α -- Prod.fst is defined implicitly
  snd : α
```
On the expression level, one of the most pervasive extensions are implicit parameters.
If a parameter is written `{x : A}` instead of `(x : A)`, Lean will attempt
to infer it from the other parameters. For example, the `List.length` function in @basic-inductives
will attempt to infer its type parameter `α` by inspecting the type of `xs`. Parameters written as
`[x : A]` are resolved using type class inference. A type class is an ordinary `inductive` or
`structure` marked as a `class`. Type class inference performs a Prolog-like search
@tabled-typeclass through definitions of class types that have been marked as an `instance`.

The type class system is frequently used to provide user-extensible notations.
This is usually done by introducing a type class with a function field together with a notation that
maps to this function. With this approach, we can implement a user-extensible addition operator as follows:
#grid(
  columns: (1fr, 1fr),
  align: center,
```lean
class Add (α : Type) where
  add : α → α → α

-- Add.add implicitly takes [Add α]
notation " + " => Add.add
```,
```lean
instance : Add Nat where
  add := Nat.add

instance : Add Int where
  add := Int.add
```
)

#pagebreak(weak: true)

== Nunchaku <sect_nunchaku>
Nunchaku @nunchakudtt is an open-source finite model finder. It is the spiritual successor
to Nitpick and was developed by Simon Cruanes and Jasmin Blanchette at INRIA.
Unlike Nitpick, Nunchaku is not tied to Isabelle as the input language and Kodkod @kodkod
as the backend solver. Instead, it consumes an idiosyncratic variant of classical @HOL @holbook and
has a family of backend solvers consisting of Kodkod, CVC4 @cvc4, SMBC @smbc, and Paradox @paradox.
Unfortunately, no publication about the overall design of Nunchaku has been made to date.
Therefore, the following section is largely based on reading the implementation and talking to
its authors.

Nunchaku's type and term language is based on @HOL, enriched with several built-in concepts. The
relevant fragment for this work is given by the following grammar:
$
  alpha ::=& type | prop | x | alpha -> alpha | c " " overline(alpha) | Pi x . alpha & "   " & "types"  \
  t ::=& c | x | t " " t | lambda x : alpha . t | "let" x : alpha := t; t | forall x : alpha . t | exists x : alpha . t \
    & | top | bot | not t | t or t | t and t | t => t | t = t | "if" t "then" t "else" t & "   " & "terms" \
$

The terms $t$ range over the usual connectives of simply typed lambda calculus, extended with
basic quantifiers and propositional connectives. The types $alpha$ consist of the universe of types,
the universe of propositions, type variables, function types, constants applied to type arguments ($overline(alpha)$
denotes a sequence of type arguments), and abstraction over type arguments.

The primary commands used to describe a Nunchaku problem are `val`, `axiom`, and `goal`.
The `val` command introduces uninterpreted constants for which Nunchaku must construct a model.
This model must ensure that the conjunction of all `axiom` predicates entails the conjunction of all
`goal` predicates.

For defining interpreted constants, Nunchaku has three main commands: `pred`, `data`, and `rec`.
Both `data` and `rec` operate similarly to Haskell, with `data` allowing definitions of polymorphic
algebraic data types and `rec` allowing definitions of recursive functions by listing equations
they must fulfill. Lastly, `pred` provides a way to define inductive predicates by specifying their
introduction rules, similar to Isabelle or Lean. Using these basic commands, we can ask
Nunchaku for a counter-model of "adding two odd numbers yields an odd one":

#grid(
  columns: (1.1fr, 0.9fr),
  align: center,
```nun
data nat := Z | S nat.

rec add : nat -> nat -> nat :=
  forall x. add Z x = x;
  forall x y. add (S x) y = S (add x y).

pred odd : nat -> prop :=
  odd (S Z);
  forall n. odd n => odd (S (S n)).
```,
```nun
val n : nat.
axiom odd n.

val m : nat.
axiom odd m.

goal ~odd (add n m).
```
)
For this problem Nunchaku quickly identifies a solution
```nun
SAT: { val m := S Z. val n := S Z. }
```

To find these solutions, Nunchaku applies long pipelines of reduction passes that
end up in the input logics of its solvers. With the CVC4 pipeline alone containing 21 passes,
explaining the pipelines in full would be far out of scope for this work. Instead, the following
description focuses on the key steps relevant for the design of the reduction from Lean to
Nunchaku.

The first step across all pipelines is the elimination of polymorphism through monomorphization. The monomorphizer
removes all polymorphism by creating specialized copies of polymorphic constants, instantiated with
the type arguments they are used with. To simplify this process, Nunchaku imposes two restrictions
on its polymorphism. First, it supports only ML-style rank-1 polymorphism, meaning that a function
can be polymorphic, but it cannot take an argument that is itself polymorphic. Second, it does
not support higher-kinded types: $Pi$ may abstract over types but not type constructors.

After eliminating polymorphism, Nunchaku performs a few additional passes to remove other
convenience features before arriving at the specialization pass. Specialization is an optimization
that attempts to eliminate higher-order functions from the problem. It does so by first identifying
higher-order arguments of `rec` functions that remain fixed in all recursive calls. Once these arguments
are identified, Nunchaku inspects the call sites of the `rec`s and generates copies with the
concrete higher-order arguments substituted in.

In the following example specialization can eliminate the higher-order function `map`:
```nun
rec map : (nat -> nat) -> list -> list :=
  forall f. map f Nil = Nil;
  forall f x xs. map f (Cons x xs) = Cons (f x) (map f xs).

val xs : list.
goal map (add Z) xs != xs.
```
As we can see in the definition of `map`, the higher-order argument `f` always remains fixed in
recursive calls. For this reason, Nunchaku decides to create a copy of `map`, specialized on the
`add Z` function. This transformation turns the problem into an entirely first-order one:
```nun
rec map_spec : list -> list :=
  map_spec Nil = Nil;
  forall x xs. map_spec (Cons x xs) = Cons (add Z x) (map_spec xs).

val xs : list.
goal map_spec xs != xs.
```

The last important pass for this work is the elimination of `pred` into `rec`. It is heavily
inspired by the encoding of inductive predicates used by Nitpick @nitpickpred. The encoding is
based on the fact that given an inductive predicate $p$ of the form
$
  & pred p : alpha_1 -> ... -> alpha_m -> prop where \
  & forall overline(y)_1 . p " " overline(t)_11 and ... and p " " overline(t)_(1cal(l)_1) and Q_1 => p " " overline(u)_1 \
  & dots.v \
  & forall overline(y)_n . p " " overline(t)_(n 1) and ... and p " " overline(t)_(n cal(l)_n) and Q_n => p " " overline(u)_n\
$
where the arguments $t_(i j)$ to $p$ and the side conditions $Q_i$ do not refer to $p$, $p$ is
equivalent to the least fixpoint of the equation @paulson-indpred @harrison-indpred #footnote[Due to the mentioned syntactic restrictions
this fixpoint always exists by the Knaster-Tarski theorem]
$
  p " " overline(x) = (exists overline(y). or.big_(j = 1)^n overline(x) = overline(u)_j and p " " overline(t)_(j 1) and ... and p " " overline(t)_(j cal(l)_j) and Q_j)
$
While we could already take this equation as the definition of $p$, this is generally unsound because
it underspecifies $p$. However, there are two cases for which this is sound.

The first case concerns negative occurrences of $p$. To identify these, Nunchaku performs a
preprocessing step called polarization before predicate elimination. The polarizer traverses the
problem and determines the polarity in which applications of inductive predicates occur.
Each predicate $p$ is then replaced by two variants, $p^-$ and $p^+$, used in negative and
positive contexts, respectively.

During predicate elimination, occurrences of $p^-$ get transformed into `rec` definitions whose
bodies correspond to the right-hand side of the fixpoint equation. This transformation is sound
because $p$ represents a least fixpoint and is thus overapproximated by $p^-$, which admits any fixpoint:
$forall overline(x) . p " " overline(x) => p^- " " overline(x)$. From this, soundness for negative contexts follows
by contraposition: $forall overline(x) . not p^- " " overline(x) => not p " " overline(x)$.

The second case concerns the remaining positive occurrences of $p$. If the recursion in the
corresponding fixpoint equation is well-founded, the equation has exactly one solution @harrison-indpred.
This allows us to use the fixpoint equation as the definition for $p^+$ as well. The recursion is
well-founded if there exists a relation $R$ such that
$
  and.big_(i=1)^n and.big_(j=1)^(cal(l)_i) (Q_i => (overline(t)_(i j), overline(u)_i) in R)
$
This is where Nunchaku takes a deviation from Nitpick. In Nitpick, the system itself will attempt to
find such a relation, while Nunchaku relies on the input problem to mark well-founded predicates.
The motivation for this design choice is that Nunchaku’s frontend may possess more domain-specific information
to establish well-foundedness than a generic mechanism within Nunchaku could reasonably infer.

Lastly, for inductive predicates that occur positively and do not fulfill the well-foundedness
criterion, Nunchaku converts them into well-founded ones. This is done by introducing an additional
fuel parameter of type $"nat"$ that decreases in each recursive call to $p^+$.
Along with this parameter, it introduces another uninterpreted constant $p^+_"decr" : "nat"$ that is
added to each application of $p^+$, specifying the initial fuel. Because the fuel decreases in every call, the recursion is
guaranteed to be well-founded and the predicate can be eliminated.

As an example for this consider the $<=$ predicate on natural numbers:
```nun
pred nat_le : nat -> nat -> prop :=
  forall n. nat_le n n;
  forall n m. nat_le n m => nat_le n (S m).
```
For negative occurrences this yields
```nun
rec nat_le- : nat -> nat -> prop :=
  forall l r . nat_le- l r =
    exists m. l = r || (r = S m && nat_le- l m).
```
If the predicate would be (correctly) marked as `[wf]` by the problem, the body of `nat_le-`
would also be used for `nat_le+`. Given that it is not marked as `[wf]` here, Nunchaku also
generates
```nun
rec nat_le+ : nat -> nat -> nat -> prop :=
  forall l r. nat_le+ Z l r = false;
  forall f l r. nat_le+ (S f) l r =
    exists m. l = r || (r = S m && nat_le+ f l m).
```
After all the reduction passes have finished, Nunchaku passes the problem to its backend
solvers. These solvers might give up on the problem, prove that it's valid, or return a
counterexample. However, this counterexample is within the logic of the solver and thus not
directly helpful to a Nunchaku user after being obscured by so many reductions. To recover the counterexample,
each of Nunchaku's reductions provides a way to translate a counterexample from its output back into
its input logic. By composing all of these recovery procedures, Nunchaku can present the counterexample
in terms of the original input problem.

#pagebreak(weak: true)

= Reduction from Lean to Nunchaku <sect_reduct>
Designing a reduction from Lean's logic into Nunchaku's requires addressing the two key
differences between them: Lean's dependent types and Lean's more expressive polymorphism.
Instead of attempting to handle both of these in full generality, this section identifies
a fragment of Lean that is reasonably easy to translate and defines a reduction for this
fragment.

The differences between Lean's and Nunchaku's polymorphism are manifold. On the tamer side, Lean
supports many constructs that are present in other functional languages such as Haskell.
For example, it allows higher-ranked polymorphism, meaning that a function may take another
polymorphic function as an argument, e.g., `Nat → ((α : Type) → α → α) → Nat`. Lean also supports
higher-kinded types, which enable abstraction over type constructors, e.g.,
`(f : Type → Type) → f a → (a → b) → f b`. Finally, Lean provides existential types,
allowing constructors to bind type arguments, e.g., a type `Dynamic : Type 1` may have a constructor
of type `(α : Type) → α → Dynamic`.

Beyond these more "common" constructs, Lean also allows for arbitrary computation with types. For example,
we can define the type of $n$-ary products of natural numbers:
```lean
def NProd (n : Nat) : Type :=
  match n with
  | 0 => Unit
  | n + 1 => Nat × NProd n
```
Lastly, Lean makes it non-trivial to identify whether a function even has a type parameter by
unifying `Prop` and `Type u` in `Sort u`.

Instead of trying to encode all of these constructs into Nunchaku's logic, I will now define a
smaller fragment of Lean. This fragment will serve as the input for the reduction to Nunchaku.
The rationale for this is that, while all of these features are sometimes
useful, many Lean formalizations do not require them at all. Thus, handling a smaller fragment of
Lean can still yield a tool that is useful for many Lean users.

This fragment is akin to a "Lean with ML-style polymorphism". It supports only
rank 1 polymorphism, maintains a clearly decidable separation of types, values, propositions, and
proofs, and forbids type-producing functions. The consequence of these requirements for
the expression syntax is the need for a clear distinction between $Prop$ and $Type u$, given by the following
grammar:
$
Gamma & ::= Gamma, x : e | epsilon & "   " & "contexts" \
e &::= x | c | e " " e | lambda x : e . e | (x : e) -> e | Prop | Type u & "   " & "expressions"
$
Because this is a mere syntactic restriction of Lean's core calculus, we can reuse Lean's type
system with the typing judgment $Gamma tack e : e$.
In the following description of the further restrictions on the fragment, I will use $x, y, z$ to
denote variables; $alpha, beta, gamma$ to denote type expressions; $s, t, u$ to denote value
expressions; and $e$ to denote expressions that might be both. This convention carries no semantic meaning on its
own and is merely meant to aid in reading. However, the restrictions put on the expressions will enforce
a match between insinuated and actual meaning. When convenient, I will denote the dependent function type as
$forall x : alpha. t$ instead.

For constants, the fragment uses the previously introduced definitions and inductive types. To avoid
working with Lean's desugaring of recursive definitions, I will only consider definitions by
a list of equations, like Nunchaku. In practice these equations are automatically computed (and verified)
by Lean upon the declaration of a new definition.
#grid(
  columns: (1fr, 1fr),
  align: center,
$
& definition c : alpha where \
& forall overline(x)_1 : overline(beta)_1. c " " overline(e)_1 = u_1 \
& dots.v \
& forall overline(x)_n : overline(beta)_n. c " " overline(e)_n = u_n \
$,
$
& inductive c " " (overline(x) : overline(alpha)) : beta where \
& "ctor"_1 : (overline(y)_1 : overline(gamma)_1) -> c " " overline(x) " " overline(t)_1 \
& dots.v \
& "ctor"_n : (overline(y)_n : overline(gamma)_n) -> c " " overline(x) " " overline(t)_n \
$
)
Observe that the type of an inductive is split into two parts: $(overline(x) : overline(alpha))$ and
$beta$. The $overline(x)$ part are the _parameters_ of $c$ and remains fixed across all
constructors. If $beta$ contains additional arguments, they are called the _indices_ of $c$ and may vary across
constructors. Together, they are called the _arguments_ of $c$.

To impose the restriction of rank-1 polymorphism on this calculus, I use a notion of
monotypes $tack alpha "mono"$ and polytypes $tack alpha "poly"$, similar to Jones et al. @higherrankedspj.
Because types can depend on terms, defining them also requires a notion of monoterms $tack t "monot"$: 

#align(center, [
  #box(proof-tree(inf-rule(
    $tack c "mono"$,
    $c "is an inductive type"$,
  )))
  #box(proof-tree(inf-rule(
    $tack x "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack Prop "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack alpha " " beta "mono"$,
    $tack alpha "mono"$,
    $tack beta "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack alpha " " t "mono"$,
    $tack alpha "mono"$,
    $tack t "monot"$,
  )))
  #box(proof-tree(inf-rule(
    $tack t " " alpha "monot"$,
    $tack t "monot"$,
    $tack alpha "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack (x : alpha) -> beta "mono"$,
    $tack alpha "mono"$,
    $tack beta "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack alpha "poly"$,
    $tack alpha "mono"$,
  )))
  #box(proof-tree(inf-rule(
    $tack (x : alpha) -> beta "poly"$,
    $tack alpha "mono"$,
    $tack beta "poly"$,
  )))
  #box(proof-tree(inf-rule(
    $tack (x : Type u) -> beta "poly"$,
    $tack beta "poly"$,
  )))
  #box(proof-tree(inf-rule(
    $tack c "monot"$,
    $c "is a def or ctor"$,
  )))
  #box(proof-tree(inf-rule(
    $tack x "monot"$,
  )))
  #box(proof-tree(inf-rule(
    $tack t " " u "monot"$,
    $tack t "monot"$,
    $tack u "monot"$,
  )))
  #box(proof-tree(inf-rule(
    $lambda x : alpha . t "monot"$,
    $tack alpha "mono"$,
    $tack t "monot"$,
  )))
])

Using these restrictions on terms and types, I extend Lean’s usual constraints on constant
declarations to obtain the target fragment. First, the type $alpha$ of a definition must satisfy
$epsilon tack alpha : Type u$ and be a polytype; each equation $"eq"_i$ must likewise satisfy
$epsilon tack "eq"_i : Prop$ and also be a polytype.
Second, the overall type of an inductive $(overline(x) : overline(alpha)) -> beta$ must
satisfy $epsilon tack (overline(x) : overline(alpha)) -> beta : Type u$ and be a polytype, with the relaxation
that the rightmost arrow of $beta$ may produce a $"Type u"$. Furthermore, each constructor $"ctor"_i$ must
satisfy $epsilon tack (overline(x) : overline(a)) -> (overline(y)_i : overline(gamma)_i) -> c " " overline(x) " " overline(t)_i : alpha$,
where $alpha$ is either $Type u$ or $Prop$, and must additionally satisfy $tack (overline(y)_i : overline(gamma)_i) -> c " " overline(x) " " overline(t)_i "mono"$
to rule out existential types. Finally, all index arguments $overline(t)_i$ appearing in the constructor’s target type must be monoterms.

In the remainder of this section I will describe the reduction from this restricted fragment
of Lean to the input logic of Nunchaku.

== Eliminating Dependent Types <sect_trans_dtt>
The first step of the reduction is the elimination of dependent types. Doing this before handling
polymorphism is not an obvious choice. The reason for choosing to eliminate dependent types first
is simplicity. With this ordering, only one step has to deal with the complexity of dependent types,
while the other way around both steps have to.

Dependent types occur in two flavors in the input fragment. First, a function or
inductive might take an argument that is a proof. This is an issue because Nunchaku has no notion of proof terms,
so they need to be removed. Second, an inductive type may have term arguments.
If the inductive type lives in $Prop$, this is not an issue because Nunchaku's inductive
predicates allow term arguments. On the other hand, if the inductive lives in $Type u$,
the term arguments need to be removed as well.

This distinction between $Type u$ and $Prop$ generalizes to more than just inductive types.
For example, the proposition $forall (n : "Nat") (p : "Nat" -> Prop). p " " n$ is perfectly
understandable to Nunchaku, despite involving dependent types in Lean. However, the type
$(n : "Nat") -> "Fin" n$ needs to be changed, because Nunchaku cannot handle the dependency of
$"Fin"$ on $n$. This shows that the reduction only needs to remove non-propositional dependent types;
dependency in a propositional context is fine for Nunchaku.

For handling proof terms, I make use of Lean's proof irrelevance. Because the concrete value of
a proof term cannot change the semantics of a program, they can be erased. This technique of _proof
erasure_ is a well-established approach and is used by both the Lean code generator and the Rocq code
extractor @coqerasure. The core idea is to introduce a new type, $"Erased"$,
with a single value $qed : "Erased"$. When a proof term needs to be removed, it gets replaced
it with $qed$, and if a binder binds a proof, the bound type gets replaced with $"Erased"$.

The elimination of dependent inductive types expands on prior work by Cruanes and Blanchette
@nunchakudtt. Their approach takes a dependent inductive type and generates two new types, a non-dependent
version for carrying the data and an inductive predicate to restrict the shape of the data
as required by the dependent type. Then, the dependent type gets replaced with the non-dependent
one, and the predicate is enforced upon it as needed.

To generate this inductive predicate, they consider inductive types of the form
$
& inductive c " " (overline(a) : Type u) : (overline(x) : overline(alpha)) ->  Type u where \
& "ctor"_1 : (overline(y)_1 : overline(beta)_1) -> (overline(z)_1 : c " " overline(a) " " overline(s)_1) -> c " " overline(a) " " overline(u)_1 \
& dots.v \
& "ctor"_n : (overline(y)_n : overline(beta)_n) -> (overline(z)_n : c " " overline(a) " " overline(s)_n) -> c " " overline(a) " " overline(u)_n \
$
Where $overline(a)$ are the type parameters, $overline(x)$ the term arguments,
$overline(z)$ the recursive occurrences of $c$ in its constructors, and $overline(y)$ the remaining
constructor arguments. From this, they derive an inductive type $"data"_c " " overline(a) : Type
u$, which drops all term arguments, together with an invariant $"inv"_c$:
$
& inductive "inv"_c " " (overline(a) : Type u) : (overline(x) : overline(alpha)) -> "data"_c " " overline(a) -> Prop where \
& "ictor"_1 :
  (overline(y)_1 : overline(beta)_1) ->
  (overline(z)_1 : "data"_c " " overline(a) " ") ->
  ("inv"_c " " overline(a) " " overline(s)_1 " " overline(z)_1) ->
  "inv"_c " " overline(a) " " overline(u)_1  " " ("ctor"_1 " " overline(a) " " overline(y) " " overline(z)_1) \
& dots.v \
& "ictor"_n :
  (overline(y)_n : overline(beta)_n) ->
  (overline(z)_n : "data"_c " " overline(a) " ") ->
  ("inv"_c " " overline(a) " " overline(s)_n " " overline(z)_n) ->
  "inv"_c " " overline(a) " " overline(u)_n  " " ("ctor"_n " " overline(a) " " overline(y) " " overline(z)_n) \
$
The idea behind $"inv"_c$ is to have one introduction rule per constructor of $c$ to restrict the
term arguments of $c$, according to that constructor. Furthermore, all recursive occurrences of $c$
get restricted by enforcing the invariant recursively for them as well.

However, I discovered that this invariant is incorrect in at least two ways. To demonstrate this, let us consider
a Lean counterexample consisting of the types `Fin n` of natural numbers less than `n` and `VecFin n`
of lists with length `n`:
```lean
inductive Fin (n : Nat) : Type where
  | mk (val : Nat) (h : val < n)

inductive VecFin : Nat → Type where
  | nil : VecFin 0
  | cons (n : Nat) (x : Fin 0) (xs : VecFin n) : VecFin (n + 1)
```
Observe that `VecFin` only contains values of type `Fin 0`. Because there are no numbers less
than zero, we can prove in Lean that for every `xs : VecFin n`, we have `xs = nil`. The invariant generated for
`VecFin` would be
```lean
inductive Fin' : Type where
  | mk (val : Nat)

inductive VecFin' : Type where
  | nil : VecFin'
  | cons (n : Nat) (x : Fin 0) (xs : VecFin') : VecFin'

inductive VecFinInv : Nat → VecFin' → Prop where
  | nil : VecFinInv 0 VecFin'.nil
  | cons (n : Nat) (x : Fin') (xs : VecFin') (h : VecFinInv n xs)
    : VecFinInv (n + 1) (VecFin'.cons n x xs)
```
The invariant fails to restrict `x` to be a number less than zero, allowing for `VecFin'`
of any length to fulfill the invariant. This issue leads us to the first insight: The encoding must also enforce
invariants of all other constructor arguments.

The second issue concerns polymorphic types; it can be demonstrated using a version of
`VecFin` that is polymorphic over the contained type and the corresponding invariant:
```lean
inductive Vec (α : Type) : Nat → Type where
  | nil : Vec α 0
  | cons (n : Nat) (x : α) (xs : Vec α n) : Vec α (n + 1)
```
```lean

inductive Vec' (α : Type) : Type where
  | nil : Vec' α
  | cons (n : Nat) (x : Fin 0) (xs : Vec' α) : Vec' α

inductive VecInv (α : Type) : Nat → Vec' α → Prop where
  | nil : VecInv α 0 Vec'.nil
  | cons (n : Nat) (x : α) (xs : Vec' α) (h : VecInv α n xs)
    : VecInv α (n + 1) (Vec'.cons n x xs)
```
The previous counterexample can be generalized to this situation as `Vec (Fin 0) n`. Once
again, the invariant fails to restrict the `x`. However, it is unclear what the invariant for
`α` should even be to begin with. This leads us to the second insight: Each type argument
must bring along its own invariant.

Using both of these insights, we can construct more faithful invariants:
```lean
inductive FinInv : Nat → Fin' → Prop where
  | mk (n : Nat) (val : Nat) (h : val < n) : FinInv n (Fin'.mk val)

inductive VecInv (α : Type) (p : α → Prop) : Nat → Vec' α → Prop where
  | nil : VecInv α p 0 Vec'.nil
  | cons (n : Nat) (x : α) (xs : Vec' α) (h1 : p x)
    (h2 : VecInv α p xs n) : VecInv α p  (n + 1) (Vec'.cons n x xs)
```
With these invariants, given `xs : Vec (Fin 0) n` the system would generate `VecInv (Fin 0) (FinInv 0) xs n`.
This new invariant correctly restricts the occurrences of `Fin 0` and thus preserves `xs = nil`.

#let dentype(ty, ..ctx) = $[| #ctx.pos().join(", ") tack ty |]_"ty"^"T"$
#let denprop(ty, ..ctx) = $[| #ctx.pos().join(", ") tack ty |]_"ty"^"P"$
#let dentyinfer(ty, ..ctx) = $[| #ctx.pos().join(", ") tack ty |]_"ty"$
#let denterm(term, ..ctx) = $[| #ctx.pos().join(", ") tack term |]_"te"$
#let denexpr(term, ..ctx) = $[| #ctx.pos().join(", ") tack term |]_"ex"$
#let mkinv(ty, ..ctx) = $[| #ctx.pos().join(", ") tack ty |]_"inv"$

To formalize this new approach, I consider inductive types of the form
$
& inductive c " " (overline(a) : Type u) " " (overline(x) : overline(alpha)) : (overline(y) : overline(beta)) -> Type u where \
& "ctor"_1 : (overline(z)_1 : overline(gamma)_1) -> c " " overline(a) " " overline(x) " " overline(t)_1 \
& dots.v \
& "ctor"_n : (overline(z)_n : overline(gamma)_n) -> c " " overline(a) " " overline(x) " " overline(t)_n \
$
where $overline(a)$ are the type parameters, $overline(x)$ the term parameters, and $overline(y)$ the indices.
The following reduction can be made to work with the different parameter kinds occurring out of order,
though having them separate makes the presentation much simpler. For this reason I also only
consider definitions whose type arguments occur grouped in the beginning of the signature.

To perform the transformation, I make use of several auxiliary functions that will be
defined through the course of this section:
- $dentype(alpha, Gamma)$ takes a context $Gamma$ and a monotype $alpha$ and eliminates
  dependent types in $alpha$. In particular, whenever it encounters a binder for a proposition, it
  replaces the bound type with $"Erased"$.
- $mkinv(alpha, Gamma)$ takes a context $Gamma$ and a monotype $alpha$ and
  generates a new expression of type $dentype(alpha, Gamma) -> Prop$. These expressions
  are like the invariants from above but generalized to arbitrary monotypes.
- $denprop(alpha, Gamma)$ takes a context $Gamma$ and a monotype $alpha$, s.t. $Gamma
  tack alpha : Prop$. It eliminates non-propositional dependent types in $alpha$ by injecting the
  data-carrying variants of inductive types and their invariants as needed.
  This function is the entry point for reducing a proposition.
- $denterm(t, Gamma)$ takes a context $Gamma$ and a monoterm $t$ and eliminates
  dependent types in $t$.
- #align(left, block($
  denexpr(e, Gamma) = cases(
     denprop(e, Gamma) "if" Gamma tack e : Prop,
     dentype(e, Gamma) "if" Gamma tack e : Type u,
     denterm(e, Gamma) "otherwise"
   ) $))
- $Gamma tack t "proof" <-> exists alpha, Gamma tack t : alpha and Gamma tack alpha : Prop$


Furthermore, I use variants of these functions for mapping and folding over vectors:
$
  dentype(overline(alpha), Gamma) &= cases(
    epsilon "if" overline(alpha) = epsilon,
    dentype(beta, Gamma)"," dentype(overline(alpha'), Gamma) "if"
    overline(alpha) = beta"," overline(alpha'),
  ) \

  dentype((overline(x) : overline(alpha)), Gamma) &= cases(
    epsilon "if" (overline(x) : overline(alpha)) = epsilon,
    dentype((y : beta), Gamma)"," dentype((overline(x) : overline(alpha')), Gamma, (y : beta)) "if"
    (overline(x) : overline(alpha)) = (y : beta)"," (overline(x') : overline(alpha')),
  ) \
$

Using these auxiliary functions, one derives the data-carrying type $"data"_c$ of an inductive $c$
by omitting all non-type parameters and the term indices:
$
& inductive "data"_c " " (overline(a) : Type u) : Type u where \
& "ctor"'_1 : dentype((overline(z)_1 : overline(gamma)_1), (overline(a) : Type u), (overline(x) : overline(beta))) -> "data"_c " " overline(a) \
& dots.v \
& "ctor"'_n : dentype((overline(z)_n : overline(gamma)_n), (overline(a) : Type u), (overline(x) : overline(beta))) -> "data"_c " " overline(a) \
$
The reason that removing all non-type parameters works is that they cannot occur in
the types of the erased constructor parameters $overline(z)_i$.

For restricting the data-carrying type, I use the construction by Cruanes and Blanchette, modified as
described before. These modifications amount to introducing one additional predicate parameter for
each type parameter and restricting all constructor arguments:
$
  (overline(x) : overline(alpha'))","(overline(y) : overline(beta')) &= dentype((overline(x) : overline(alpha))","(overline(y) : overline(beta)) , (overline(a) : Type u)) \
  (overline(z) : overline(gamma')_i) &= dentype((overline(z)_i : overline(gamma)_i), (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop), (overline(x) : overline(alpha))) \
  overline(t')_i &= denterm(overline(t)_i, (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop), (overline(x) : overline(alpha)), (overline(z)_i : overline(gamma)_i)) \
  "inv"_(overline(x)) &= mkinv((overline(x) : overline(alpha)), (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop)) \
  "inv"_(overline(z)_i) &= mkinv((overline(z)_i : overline(gamma)_i), (overline(a) : Type u), (p_overline(a) : overline(x) -> Prop), (overline(x) : overline(alpha))) \
$
$
& inductive "inv"_c " " (overline(a) : Type u) " " (p_overline(a) : overline(a) -> Prop) " " (overline(x) : overline(alpha')) : (overline(y) : overline(beta')) -> "data"_c " " overline(a) -> Prop where \
& "ictor"_1 :
("inv"_overline(x) " " overline(x)) ->
(overline(z)_1 : overline(gamma')_1) ->
("inv"_(overline(z)_1) " " overline(z)_1) ->
"inv"_c " " overline(a) " " p_overline(a) " " overline(x) " " overline(t')_1 " " ("ctor"'_1 " " overline(a) " " overline(z)_1) \
& dots.v \
& "ictor"_n :
("inv"_overline(x) " " overline(x)) ->
(overline(z)_n : overline(gamma')_n) ->
("inv"_(overline(z)_n) " " overline(z)_n) ->
"inv"_c " " overline(a) " " p_overline(a) " " overline(x) " " overline(t')_n " " ("ctor"'_n " " overline(a) " " overline(z)_n) \
$
Next, I define $mkinv(alpha, Gamma)$ for generating an invariant for
any monotype. Doing this is necessary because we might, for example, have to restrict the co-domain
of a dependent function bound in a constructor.


#[
#set par(leading: 0.4em)
#table(
  columns: (110pt, 255pt, 100pt),
  stroke: none,
  inset: 0pt,
  row-gutter: 13pt,
  column-gutter: 0pt,
  $mkinv(alpha, Gamma)$,
  $= lambda (\_ : "Erased") . denprop(alpha, Gamma)$,
  $"if" Gamma tack alpha : Prop$,

  $mkinv(c " " overline(alpha) " " overline(e) " " overline(t), Gamma)$,
  $= "inv"_c " " dentype(overline(alpha), Gamma) " " mkinv(overline(alpha), Gamma) " " denexpr(overline(e), Gamma) " " denterm(overline(t), Gamma)$,
  $$,

  $mkinv((overline(x) : overline(alpha)) -> beta, Gamma)$,
  $=
  lambda &(f : dentype((overline(x) : overline(alpha)) -> beta, Gamma)) .
   forall(overline(x) : dentype(alpha, Gamma)) . \
     &mkinv(overline(alpha), Gamma) " " overline(x) -> mkinv(beta, Gamma,
     (overline(x) : overline(alpha))) " " (f " " overline(x))$,
  $$,

  $mkinv(Prop, Gamma)$,
  $= lambda (\_ : Prop) . top$,
  $$,

  $mkinv(x, Gamma)$,
  $= p_x$,
  $$,

)
]

When the invariant generator encounters a proposition, the proposition itself becomes the invariant.
Inductive types are handled via their associated inductive invariants. In the case of function types,
the co-domain’s invariant must hold whenever the domain invariants do. Finally, $Prop$ imposes no invariant
and type variables use their corresponding predicate variables.

Using all of this machinery, I now define the functions for removing dependent types from
monoterms and monotypes. The key question here is where to place the invariants for restricting
dependent types. Because we are only working with well-typed expressions, many occurrences of
dependent types already fulfill the required invariants by construction. For example, a term of type
$"Vec" 37$ that is only built from constructors is also a list of length $37$ after naively
removing all dependent types. The crucial exception to this is universal quantification. Whenever we
encounter universal quantification over a dependent type, we need to restrict the resulting
quantification over the data-carrying type. This ensures that the quantifier cannot be instantiated
with values that violate the invariants by Nunchaku's solvers later on.
#pagebreak(weak: true)
The issue with universal quantifiers can for example be observed in the proposition
$ forall (n : "Nat")(x : "Vec" n). "length" x = n $
If we naively substitute all types with their data-carrying types, we are left with
a statement that claims any list can have any length, which is clearly wrong:
$ forall (n : "data"_"Nat")(x : "data"_"Vec"). "length" x = n $
Instead, $x$ needs to be restricted to lists of length $n$:
$ forall (n : "data"_"Nat")(x : "data"_"Vec"). "inv"_"Vec" " " n " " x -> "length" x = n $

Thus we end up with the following definitions for translating monotypes and monoterms:

#[
#set par(leading: 0.4em)
#table(
  columns: (110pt, 250pt, 100pt),
  stroke: none,
  inset: 0pt,
  row-gutter: 13pt,
  column-gutter: 0pt,
  [*Terms*], [], [],
  $denterm(t, Gamma)$,
  $= qed$,
  $"if" Gamma tack t : "proof"$,

  $denterm(x, Gamma)$,
  $= x$,
  $$,

  $denterm(lambda (x : alpha) . t, Gamma)$,
  $= lambda (x : dentype(alpha, Gamma)) . denterm(t, Gamma, (x : alpha))$,
  $$,

  $denterm(c " " overline(alpha) " " overline(e), Gamma)$,
  $= c' " " dentype(overline(alpha), Gamma) " " mkinv(overline(alpha), Gamma) " " denexpr(overline(e), Gamma)$,
  $$,

  $denterm(t " " e, Gamma)$,
  $= denterm(t, Gamma) " " denexpr(e, Gamma)$,
  $$,

  [*Types*], [], [],

  $dentype(alpha, Gamma)$,
  $= "Erased"$,
  $"if" Gamma tack alpha : Prop$,

  $dentype(x, Gamma)$,
  $= x$,
  $$,

  $dentype(Prop, Gamma)$,
  $= Prop$,
  $$,

  $dentype((x : alpha) -> beta, Gamma)$,
  $= (x : dentype(alpha, Gamma)) -> dentype(beta, Gamma, (x : alpha))$,
  $$,

  $dentype(c " " overline(alpha) " " overline(e) " " overline(t))$,
  $= "data"_c " " dentype(overline(alpha), Gamma)$,
  $$,

  [*Propositions*], [], [],

  $denprop(x, Gamma)$,
  $= x$,
  $$,

  $denprop((x : alpha) -> beta, Gamma)$,
  $= (x : denprop(alpha, Gamma)) -> denprop(beta, Gamma, (x : alpha))$,
  $"if" Gamma tack alpha : Prop$,

  $denprop((x : alpha) -> beta, Gamma)$,
  $= &(x : dentype(alpha, Gamma)) \
     &-> mkinv(alpha, Gamma) " " x \
     &-> dentype(beta, Gamma, (x : alpha))$,
  $"if" Gamma tack alpha : Type u$,

  $denprop(c " " overline(alpha) " " overline(e) " " overline(t))$,
  $= c' " " dentype(overline(alpha), Gamma) " " mkinv(overline(alpha), Gamma) " " denexpr(overline(e), Gamma) " " denterm(overline(t), Gamma)$,
  $$,
)
]

This leaves only the translation of inductive propositions and definitions.
Translating inductive propositions is quite similar to generating invariants for inductive types.
However, instead of generating a separate invariant, the restrictions can be directly injected into the
introduction rules of the proposition. Given an inductive proposition of the form
$
& inductive c " " (overline(a) : Type u) " " (overline(x) : overline(alpha)) : (overline(y) : overline(beta)) -> Prop where \
& "ctor"_1 : (overline(z)_1 : overline(gamma)_1) -> c " " overline(a) " " overline(x) " " overline(t)_1 \
& dots.v \
& "ctor"_n : (overline(z)_n : overline(gamma)_n) -> c " " overline(a) " " overline(x) " " overline(t)_n \
$
its replacement proposition $c'$ is
$
  (overline(x) : overline(alpha'))","(overline(y) : overline(beta')) &= dentype((overline(x) : overline(alpha))","(overline(y) : overline(beta)) , (overline(a) : Type u)) \
  (overline(z) : overline(gamma')_i) &= dentype((overline(z)_i : overline(gamma)_i), (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop), (overline(x) : overline(alpha))) \
  overline(t')_i &= denterm(overline(t)_i, (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop), (overline(x) : overline(alpha)), (overline(z)_i : overline(gamma)_i)) \
  "inv"_(overline(x)) &= mkinv((overline(x) : overline(alpha)), (overline(a) : Type u), (p_overline(a) : overline(a) -> Prop)) \
  "inv"_(overline(z)_i) &= mkinv((overline(z)_i : overline(gamma)_i), (overline(a) : Type u), (p_overline(a) : overline(x) -> Prop), (overline(x) : overline(alpha))) \
$
$
& inductive c' " " (overline(a) : Type u) " " (p_overline(a) : overline(a) -> Prop) " " (overline(x) : overline(alpha')) : (overline(y) : overline(beta')) -> Prop where \
& "ctor"'_1 :
("inv"_overline(x) " " overline(x)) ->
(overline(z)_1 : overline(gamma')_1) ->
("inv"_(overline(z)_1) " " overline(z)_1) ->
c' " " overline(a) " " p_overline(a) " " overline(x) " " overline(t')_1 \
& dots.v \
& "ctor"'_n :
("inv"_overline(x) " " overline(x)) ->
(overline(z)_n : overline(gamma')_n) ->
("inv"_(overline(z)_n) " " overline(z)_n) ->
c' " " overline(a) " " p_overline(a) " " overline(x) " " overline(t')_n \
$

The last remaining construct are definitions. Because we only consider definitions with type
arguments grouped in the beginning, they are of the form
$
& definition c : (overline(a) : Type u) -> beta where \
& forall (overline(a) : Type u) (overline(x)_1 : overline(gamma)_1). c " " overline(a) " " overline(t)_1 = u_1 \
& dots.v \
& forall (overline(a) : Type u) (overline(x)_n : overline(gamma)_n). c " " overline(a) " " overline(t)_n = u_n \
$
Given that all inputs to function calls should already be properly constrained by invariants, there
is no need to enforce any additional invariants in the equations. Thus, the only modifications for a
definitions are the addition of the predicate parameters for type arguments and dependent type
erasure:
$
beta' &= dentype(beta, (overline(a) : Type u)) \
overline(gamma')_i &= dentype((overline(x)_i : overline(gamma)_i), (overline(a) : Type u)) \
overline(t')_i &= denterm(overline(t)_i, (overline(a) : Type u), (overline(p) : overline(a) -> Type u), (overline(x)_i : overline(gamma)_i) ) \
u'_i &= denterm(u_i, (overline(a) : Type u), (overline(p) : overline(a) -> Type u), (overline(x)_i : overline(gamma)_i) ) \
$
$
& definition c' : (overline(a) : Type u) -> (overline(p) : overline(a) -> Type u) -> beta' where \
& forall (overline(a) : Type u) (overline(p) : overline(a) -> Type u) (overline(x)_1 : overline(gamma')_1). c' " " overline(a) " " overline(t')_1 = u'_1 \
& dots.v \
& forall (overline(a) : Type u) (overline(p) : overline(a) -> Type u) (overline(x)_n : overline(gamma')_n). c' " " overline(a) " " overline(t')_n = u'_n \
$

Using all of this machinery, we can now take a monotype $alpha$ such that $epsilon tack alpha : Prop$ and
erase all non-propositional dependent types from it with $denprop(alpha, epsilon)$. This
leaves us with a logic where dependent types only occur as universal propositional quantification and
inductive propositions, both of which are supported by Nunchaku.

This concludes the presentation of the naive encoding for dependent types. However, in many cases
the invariants placed by this encoding are going to turn out unnecessary. For example, when translating
commutativity of addition on natural numbers
$ forall (n : "Nat") (m : "Nat"), n + m = m + n $
the encoding produces
$
  forall (n : "data"_"Nat"). "inv"_"Nat" " " n -> forall (m : "data"_"Nat"), "inv"_"Nat" " " m -> n + m = m + n
$
While this is sound, the $"inv"_"Nat"$ assumptions are unnecessary. After all, $"Nat"$ is not a
dependent type and also doesn't transitively contain any dependent types which leaves nothing for
$"inv"_"Nat"$ to enforce.

Due to these trivial invariants, many expressions that do not involve dependent types can be
reduced to expressions without any invariants at all. Doing so can be useful for finding
counterexamples with Nunchaku, as its solvers might have to invest effort or even fail to see that
some invariants are trivial.

To add this capability to the reduction, we need to detect types with trivial structure
and make the invariant generator return a trivial invariant when it encounters them:
$
  mkinv(alpha, Gamma) = lambda (x : dentype(alpha, Gamma)) . top "    " "if" Gamma tack alpha "trivial"
$

#align(center, [
  #box(proof-tree(inf-rule(
    $Gamma tack Prop "trivial"$,
  )))
  #box(proof-tree(inf-rule(
    $Gamma tack (overline(x) : overline(alpha)) -> beta "trivial"$,
    $Gamma tack beta "trivial"$
  )))
  #box(proof-tree(inf-rule(
    $Gamma tack c "trivial"$,
    $c "is a trivial inductive type"$,
  )))
])
Where $c$ is a trivial inductive type if it takes no parameters or indices and all of its
constructor arguments have trivial types. This criterion ensures that we only omit invariants when
encountering an inductive type that is guaranteed to have a trivial invariant, or a function to such a type.

While this final reduction is sound for many Lean problems in practice, as we are going to see
in @sect_case_studies and @sect_eval, it is not generally sound. The reason for this is that the
reduction sometimes fails to enforce invariants when they are required. For example, consider
the $"head"$ function on $"Vec"$
$
& definition "head" : (n : "Nat") -> "Vec" ("succ" n) -> "Nat" where \
& forall (n : "Nat") (x : "Nat") (t : "Vec" n). "head" n " " ("cons" n " " x " " t) = x \
$
After the reduction this function becomes
$
& definition "head"' : "data"_"Nat" -> "data"_"Vec" -> "data"_"Nat" where \
& forall (n : "data"_"Nat") (x : "data"_"Nat") (t : "data"_"Vec"). "head"' " " n " " ("cons"' " " n " " x " " t) = x \
$
Observe that this function is now underspecified; it is missing the $"nil"$ case, which was previously
unnecessary. This is not generally an issue for Nunchaku. When a function is underspecified,
Nunchaku lets the backend solvers fill in the missing parts arbitrarily. If the inputs to $"head"$ are
properly restricted, this is unproblematic because the underspecified cases are never exercised.
However, consider a case where we had defined a copy of $"head"$, called $"head"_2$.
Although $"head" = "head"_2$ is provable in Lean using function extensionality, after the reduction
both functions have an underspecified case. Because they are separate definitions, the backend
solvers of Nunchaku can fill in these underspecified cases differently and come to the conclusion
that $"head"' != "head"'_2$.

Even though simple cases like $"head" = "head"_2$ can be resolved by explicitly applying function
extensionality and enforcing invariants on the function inputs, this is not generally possible.
Equality between underspecified functions might be hidden behind polymorphism. For example, consider a
predicate $"P" " " (a : "Type u") : a -> a -> Prop$ with a single introduction rule
$"intro" : (l : a) -> (r : a) -> l = r -> "P" " " a " " l " " r$. Instead of exposing the equality
between functions directly, it could be hidden behind this predicate:
$ "P" " " ((n : "Nat") -> "Vec" ("succ" n) -> "Nat") "head" "head"_2 $
In these situations, it is not possible to apply function extensionality because the equality is
between values of type $a$ and not directly between functions. A potential fix for this could be
to eliminate polymorphism before eliminating dependent types. Note that this would come at the cost
of additional complexity because both steps would have to handle dependent types.

== Eliminating Polymorphism <sect_trans_poly>
With dependent types out of the way, we are left with a logic with rank-1 polymorphism. In
principle, this logic can be translated to Nunchaku immediately, since Nunchaku supports rank-1 polymorphism.
However, as explained previously, Lean supports much more expressive variants of
polymorphism. While they are currently ignored, it would be better to use an approach that can later
be extended to support a larger fragment of Lean as well. Recent work by Lutze et al. @monotypeflow
provides such an extensible framework.

Lutze et al. use a type flow analysis to monomorphize both higher-ranked and existential
polymorphism. The analysis proceeds in three phases to monomorphize a program.
First, it traverses the program and generates constraints based on what types
polymorphic constants are directly applied to. Then, it determines whether these constraints admit a finite
solution, and if they do, it determines a solution through fixpoint iteration. This solution is a map
from type variables to the set of types with which they might be instantiated. Lastly, the analysis
traverses the program again and instantiates each polymorphic constant according to the solution.

In the remainder of this section, I present an adaptation of their type flow analysis to the
flavor of rank-1 polymorphism required for this work. To simplify the presentation, I only describe
the analysis for problems where each constant has at most one type parameter (just like Lutze et al.).
In practice, the implementation supports any number of type variables.

The analysis tracks constraints of the form $tau subset.sq.eq x$,
pronounced "monotype $tau$ flows into variable $x$". These constraints only need to
handle monotypes that live in $Type u$, and dependent types have been eliminated. For
this reason, both the types in the constraints and the type parameters to polymorphic constants can
only take on four forms:
$ tau ::= x | tau_1 -> tau_2 | c " " tau | c $

For collecting the constraints, I define a function $"C"$ that operates on both types and
terms. Similar to the dependent type elimination, this function assumes that the type arguments
are grouped in the beginning of a constant application. Furthermore, to simplify the presentation,
it also assumes that the names of all type variables are globally unique. In practice this can be
achieved by disambiguating them through the name of the polymorphic constant they are attached to:

#let coll(e) = $"C"(#e)$

#[
#set par(leading: 0.4em)

#table(
  columns: (90pt, 120pt, 300pt),
  stroke: none,
  inset: 0pt,
  row-gutter: 13pt,
  column-gutter: 0pt,
  [*Terms*], [], [],
  $coll(x)$,
  $= {}$,
  $$,

  $coll(c)$,
  $= {}$,
  $$,

  $coll(lambda (x : alpha) . t)$,
  $= coll(alpha) union coll(t)$,
  $$,

  $coll(c " " alpha)$,
  $= {alpha subset.eq.sq x} union C(alpha)$,
  $"if" c "is a def with type argument" x$,

  $coll(c " " alpha)$,
  $= {alpha subset.eq.sq x} union C(alpha)$,
  $"if" c "is a constructor of an inductive with type argument" x$,

  $coll(t " " e)$,
  $= coll(t) union coll(e)$,
  $$,

  [*Types*], [], [],
  $coll(x)$,
  $= {}$,
  $$,

  $coll(c)$,
  $= {}$,
  $$,

  $coll(Prop)$,
  $= {}$,
  $$,

  $coll(Type u)$,
  $= {}$,
  $$,

  $coll((x : alpha) -> beta)$,
  $= coll(alpha) union coll(beta)$,
  $$,

  $coll(c " " alpha)$,
  $= {alpha subset.eq.sq x} union C(alpha)$,
  $"if" c "is an inductive with type argument" x$,

  $coll(alpha " " e)$,
  $= coll(alpha) union coll(e)$,
  $$,
)
]

Note that $"C"$ still has to consider the dependent arrow and term arguments to types because that
is how propositions are encoded. In addition to the constraints collected from terms and types
directly, constant declarations naturally impose some constraints too. Again, due to
propositional dependent types, these constraints have to take the possibility of dependently typed
inductives into account:
#table(
  columns: (160pt, 100pt, 230pt),
  stroke: none,
  align: horizon,
  inset: 0pt,
  row-gutter: 10pt,
  align(left, block(
  $
  & inductive c " " (overline(x) : overline(alpha)) : beta where \
  & "ctor"_1 : (overline(y)_1 : overline(gamma)_1) -> c " "overline(x) " " overline(t)_1 \
  & dots.v \
  & "ctor"_n : (overline(y)_n : overline(gamma)_n) -> c " "overline(x) " " overline(t)_n \
  $)),
  $ arrow.r.squiggly $,
  align(left, block(
  $ C((overline(x) : overline(alpha)) -> beta) union union.big_(i=0)^n C((overline(y)_i : overline(gamma)_i) -> c " "overline(x) " " overline(t)_i) $
  )),
  align(left, block(
  $
  & definition c : alpha where \
  & forall overline(x)_1 : overline(beta)_1. c " " overline(e)_1 = u_1 \
  & dots.v \
  & forall overline(x)_n : overline(beta)_n. c " " overline(e)_n = u_n \
  $)),
  $ arrow.r.squiggly $,
  align(left, block(
  $ coll(alpha) union union.big_(i=0)^n (coll(overline(beta)_i) union coll(c " " overline(e)_i) union coll(u_i)) $
  ))
)

Using this scheme, we can, for example, build the constraint system of a simple term
$"add" ("length" "Nat" x_1) " " ("length" "String" x_2)$, where $"length"$ is defined as follows:
#align(left, block(
$
& inductive "List" (a : Type) : Type where \
& "nil" : "List" a \
& "cons" : a -> "List" a -> "List" a \
$))
#align(left, block(
$
& definition "length" : (b : Type) -> "List" b -> "Nat" where \
& forall (b : Type). "length" b "nil" = "zero" \
& forall (b : Type) (h : b) (t : "List" b). "length" b " " ("cons" h " " t) = "succ" ("length" b " " t)  \
$))
This results in the constraints ${"Nat" subset.eq.sq b, "String" subset.eq.sq b, b subset.eq.sq a, b
subset.eq.sq b, a subset.eq.sq a}$, which have a solution with ${a |-> {"Nat", "String"}, b |-> {"Nat", "String"}}$.
To monomorphize this problem, we would have to build copies of $"List"$ and $"length"$ instantiated with each of the
solutions for their type variables. However, a finite solution cannot always be found.

The reason that we may fail to find a finite solution is polymorphic recursion.
Polymorphic recursion occurs whenever recursive occurrences of a constant have other type parameters
than the constant itself. For example, binary trees encoded using polymorphic pairs exhibit
polymorphic recursion:
#grid(
  columns: (1fr, 1fr),
  align: center,
$
& inductive "Tree" (a : Type) : Type where \
& "leaf" : a -> "Tree" a \
& "node" : "Tree" ("Two" a) -> "Tree" a \
$,
$
& inductive "Two" (b : Type) : Type where \
& "mk" : b -> b -> "Two" b \
$,
)
These type declarations give rise to the constraints ${"Two" a subset.eq.sq a, a subset.eq.sq b, a subset.eq.sq a, b subset.eq.sq b}$.
Here the constraint $"Two" a subset.eq.sq a$ presents an issue because it requires instantiating $a$
with an infinite set of types of the form $"Two" a, "Two" ("Two" a), "Two" ("Two" ("Two" a))$, and so on.
This makes monomorphization for this problem infeasible because we cannot generate infinitely many
specialized copies of $"Tree"$.

In order to detect these, rather rare, situations, Lutze et al. convert the constraints into a directed graph and
detect harmful cyclic flows within the graph. For a given set of constraints $R$, this graph $G = (V, E)$ can
be derived as follows:
$
  V &= { x | tau subset.eq.sq x in R } \
  E &= {(y, x) | y in "fv"(tau) and tau subset.eq.sq x in R } \
$
#grid(
  columns: (0.6fr, 1fr, 0.6fr, 0.6fr),
  align: center,
  $"fv"(x) = {x}$,
  $"fv"(tau_1 -> tau_2) = "fv"(tau_1) union "fv"(tau_2)$,
  $"fv"(c " " tau) = "fv"(tau)$,
  $"fv"(c) = {}$,
)
Furthermore, all potentially problematic edges in the graph are marked:
$
"mark"((y, x)) = cases(top "if" exists tau\, tau != y and y in "fv"(tau) and tau subset.eq.sq x in R, bot "otherwise")
$
The constraint system $R$ is then solvable iff there exists no $e in E$ with $"mark"(e) = top$ that is
on a cycle in $G$.

In their artifact @typeflowartifact, Lutze et al. use repeated BFS from the destination
node to the origin node of each marked edge to search for such a cycle. However, this is wasteful as
it might repeatedly explore parts of the graph unnecessarily. Instead, I use an algorithm based on strongly connected
components. #footnote[The idea of using SCCs for this was given to me by Siddharth Bhat in private communications]
It uses the fact that two vertices are on a cycle iff they are in the same SCC.
The algorithm assumes the existence of a subroutine $"SCC"(G)$ that computes a map from each node of
$G$ to a unique identifier of the SCC it is a member of. This can be done using Trajan's
algorithm in time $O(abs(V) + abs(E))$. Overall, the run time of the algorithm is bounded by the SCC
finding and thus $O(abs(V) + abs(E))$.
#pseudocode-list(booktabs: true)[
  - *input*:
    - constraint graph $G$
    - precomputed marking table $"mark"$
  - *output* true if the constraint system behind $G$ is solvable, false otherwise
  + $"scc" = "SCC"(G)$
  + *for* $(y, x)$ *in* $E$ *do*
    + *if* $"mark"((y, x)) = top and "scc"(y) = "scc"(x)$ *then*
      + *return* false
  + *return* true
]

Once solvability has been established, the constraint problem $R$ can be converted into a fixpoint
problem as follows:
$
  x &= x union union.big{ "inst"(tau) | tau subset.sq.eq x in R } "   for every variable" x "in" R \
$
#grid(
  columns: (0.3fr, 0.7fr),
  row-gutter: 1em,
  align: center,
  $
   "inst"(x) &= x \
   "inst"(c) &= {c}
  $,
  $
    "inst"(tau_1 -> tau_2) &= { tau'_1 -> tau'_2 | (tau'_1, tau'_2) in "inst"(tau_1) times "inst"(tau_2) } \
    "inst"(c " " tau) &= { c " " tau' | tau' in "inst"(tau) }
  $
)
Because the constraint problem is solvable, this fixpoint problem is guaranteed to admit a finite solution
set for every type variable. The types occurring in these sets are free of type variables and are
thus called ground types:
$ rho ::= rho_1 -> rho_2 | c " " rho | c $
A solution for a set of constraints can be interpreted as a map $S$ from type variables to the set
of ground types $S(x)$ that each type variable may be instantiated with.

Using this solution $S$, we can monomorphize the original problem that gave rise to the constraint
system. Similarly to constraint collection, this step uses a function $"M"$ on types and terms and
performs transformations on constant declarations.

#let mon(e) = $"M"(#e)$

#[
#set par(leading: 0.4em)

#table(
  columns: (90pt, 160pt, 90pt, 120pt),
  stroke: none,
  inset: 0pt,
  row-gutter: 13pt,
  column-gutter: 0pt,
  [*Terms*], [], [*Types*], [],
  $mon(x)$,
  $= x$,
  $mon(c)$,
  $= c$,

  $mon(c)$,
  $= c$,
  $mon(Prop)$,
  $= Prop$,

  $mon(lambda (x : alpha) . t)$,
  $= lambda (x : mon(alpha)) . mon(t)$,
  $mon(Type u)$,
  $= Type u$,

  $mon(c " " rho)$,
  $= c_rho$,
  $mon((x : alpha) -> beta)$,
  $= (x : mon(alpha)) -> mon(beta))$,

  $mon(t " " e)$,
  $= mon(t) " " mon(e)$,
  $mon(c " " rho)$,
  $= c_rho$,

  $$,
  $$,
  $mon(alpha " " e)$,
  $= mon(alpha) " " mon(e)$,
)]

For a constant without type arguments, monomorphization amounts to running $"M"$ on its type and
constructors or equations. For a polymorphic constant $c$ with type argument $x$, we need to generate a
copy for every $rho in S(x)$. I use the notation $e[x |-> rho]$ for substituting a type variable $x$
in an expression $e$ with a ground type $rho$.
#table(
  columns: (230pt, 30pt, 240pt),
  stroke: none,
  align: horizon,
  row-gutter: 20pt,
  inset: 0pt,
  align(left, block(
  $
  & inductive c " " (a : Type u) " " (overline(x) : overline(alpha)) : beta where \
  & "ctor"_1 : (overline(y)_1 : overline(gamma)_1) -> c " " a " " overline(x) " " overline(t)_1 \
  & dots.v \
  & "ctor"_n : (overline(y)_n : overline(gamma)_n) -> c " " a " " overline(x) " " overline(t)_n \
  $)),
  $ arrow.r.squiggly $,
  align(left, block(
  $
  & inductive c_rho " " mon(((overline(x) : overline(alpha)) : beta)[a |-> rho]) where \
  & "ctor"_1_rho : mon(((overline(y)_1 : overline(gamma)_(1))  -> c " " a " " overline(x) " " overline(t)_1)[a |-> rho]) \
  & dots.v \
  & "ctor"_n_rho : mon(((overline(y)_n : overline(gamma)_(n)) -> c " " a " " overline(x) " " overline(t)_n)[a |-> rho])\
  $)),
  align(left, block(
  $
  & definition c : (a : Type u) -> alpha where \
  & forall (a : Type u) (overline(x)_1 : overline(beta)_1). c " " a " " overline(e)_1 = u_1 \
  & dots.v \
  & forall (a : Type u) (overline(x)_n : overline(beta)_n). c " " a " " overline(e)_n = u_n \
  $)),
  $ arrow.r.squiggly $,
  align(left, block(
  $
  & definition c_rho : mon(alpha[a |-> rho]) where \
  & mon((forall (overline(x)_1 : overline(beta)_1). c " " a " " overline(e)_1 = u_1)[a |-> rho]) \
  & dots.v \
  & mon((forall (overline(x)_n : overline(beta)_n). c " " a " " overline(e)_n = u_n)[a |-> rho]) \
  $)),
)

With this monomorphization procedure, we can eliminate polymorphism from the previous
$"length"$ example with the solution ${a |-> {"Nat", "String"}, b |-> {"Nat", "String"}}$.
This solution requires us to build two copies of each base constant. Thus the original polymorphic constants
#align(left, block(
$
& inductive "List" (a : Type) : Type where \
& "nil" : "List" a \
& "cons" : a -> "List" a -> "List" a \
$))
#align(left, block(
$
& definition "length" : (b : Type) -> "List" b -> "Nat" where \
& forall (b : Type). "length" b "nil" = "zero" \
& forall (b : Type) (h : b) (t : "List" b). "length" b " " ("cons" h " " t) = "succ" ("length" b " " t)  \
$))
get converted into four non-polymorphic constants:
#grid(
  columns: (1fr, 1fr),
align(left, block(
$
& inductive "List"_"Nat" : Type where \
& "nil"_"Nat" : "List"_"Nat" \
& "cons"_"Nat" : "Nat" -> "List"_"Nat" -> "List"_"Nat" \
$)),
align(left, block(
$
& inductive "List"_"String" : Type where \
& "nil"_"String" : "List"_"String" \
& "cons"_"String" : "String" -> "List"_"String" -> "List"_"String" \
$))
)


#align(left, block(
$
& definition "length"_"Nat" : "List"_"Nat" -> "Nat" where \
& "length"_"Nat" " " "nil"_"Nat" = "zero" \
& forall (h : b) (t : "List"_"Nat"). "length"_"Nat" " " ("cons"_"Nat" " " h " " t) = "succ" ("length"_"Nat" " " b " " t)  \
$))
#align(left, block(
$
& definition "length"_"String" : "List"_"String" -> "Nat" where \
& "length"_"String" " " "nil"_"String" = "zero" \
& forall (h : b) (t : "List"_"String"). "length"_"String" " " ("cons"_"String" " " h " " t) = "succ" ("length"_"String" " " b " " t)  \
$))

And finally the original term $"add" ("length" "Nat" x_1) " " ("length" "String" x_2)$ gets
converted into $"add" ("length"_"Nat" " " x_1) " " ("length"_"String" " " x_2)$.


After applying this transformation, we are left with a logic with only propositional dependent types
and no type parameters. This logic can be easily translated into Nunchaku's input format with a
few syntactic manipulations. Furthermore, the algorithm presented in this section can be extended to
more variants of polymorphism in the future by extending $"C"$ and $"M"$ as required.

#pagebreak(weak: true)

= Implementation <sect_impl>
While the previous section provides a theoretical approach for finite model finding in Lean, there
are still additional steps required to integrate this approach with Lean. In this section I describe both the
implementation on the Lean side and the modifications I made to Nunchaku for improving its performance
on the generated problems.

== The Lean Frontend <sect_impl_lean>
On the Lean side, I implemented a new tactic called Chako #footnote[https://github.com/hargoniX/chako-lean]. Chako can be called at any point during a
Lean proof and will attempt to translate the current proof state to Nunchaku's logic. If this
succeeds, it invokes Nunchaku to search for a counterexample, up to a default timeout of ten seconds.
When Nunchaku finds a counterexample, it is translated backwards along the pipeline steps of
Chako to provide a readable counterexample to the user.

Given that Chako, just like Nunchaku, is in essence a pipeline of reduction steps with a forwards
and backwards translation, I adapted Nunchaku's pipeline type to Lean:
```lean
structure TransformationInner (a b c d st : Type) where
  name : String
  encode : a → TransforM (b × st)
  decode : st → c → TransforM d

structure Transformation (a b c d : Type) where
  {st : Type}
  inner : TransformationInner a b c d st

inductive Pipeline : Type → Type → Type → Type → Type 1 where
  | tip (trans : Transformation a b c d) : Pipeline a b c d
  | compose (trans : Transformation a b e f) (pipe : Pipeline b c d e)
    : Pipeline a c d f
```
Each pipeline step has four type arguments, consisting of the input and output type for its encoding
step, as well as the input and output type for its decoding step. Because these types are tracked,
pipelines can be composed in a type-safe manner using the `compose` constructor. In addition
to these four type arguments, each `Transformation` carries an existential type `st`. Values of this
type are part of the output of the encoding step. Later these values are fed back into the
decoding step. Each pipeline step can use its `st` to store information generated during the
encoding that is useful for decoding a counterexample. For example, the monomorphization step
stores which polymorphic constant each monomorphized constant is derived from.

In the implemented encoding pipeline, none of the pipeline steps ever leaves Lean's own expression
representation. This has several benefits. For one, all pipeline steps can make use of Lean's
powerful metaprogramming API for type inference, reduction, working with binders, etc. Furthermore, the Lean kernel is able to
verify that each of the pipeline steps did at least produce something type correct. This is
useful for finding bugs while making changes to the implementation. However, type checking can make
the system slow during actual use and can thus be disabled with an option. Lastly, Lean allows
building fast caches keyed by expressions in a simple manner. This does, for example, enable caching which subterms are proof terms
in an efficient and easy manner.

The pipeline of Chako consists of five steps. First, it infers information
necessary for the translation. Afterward, it cleans up the proof state before
invoking the dependent type elimination and monomorphization from @sect_reduct. The resulting problem
is then handed off to Nunchaku through a simple syntactic translation.

Before starting the translation, Chako collects two important pieces of information. First, it
derives all the equations for the definitions involved in the current proof state. This is
done with Lean's `Meta.getEqnsFor?` facility for the most part. However, this API does not offer equations for
all definitions. The most notable exceptions are "matchers" and "casesOn", which are used to
encode nested pattern matching in other equations built by `Meta.getEqnsFor?`. For these two groups
of definitions, Chako has special support for synthesizing its own equations.

After this, Chako attempts to infer for each inductive predicate whether it is well-founded. For
this, it follows the same approach as Nitpick: Translate the well-foundedness problem to a termination
problem and use the theorem prover's built-in termination prover to establish well-foundedness. Because
Lean's termination prover is equipped with some domain-specific knowledge, this process can even
establish well-foundedness for inductive predicates such as
```lean
inductive Below (p : Nat → Prop) : Nat → Prop where
  | here (n : Nat) (h : p n) : Below p n
  | lower (n m : Nat) (h1 : n < m) (h2 : Below p n) : Below p m
```
Each of the pipeline steps propagates which inductive predicates are well-founded to the next one.
When outputting the Nunchaku problem, they are marked as `[wf]` accordingly.

With all the relevant information collected, the pipeline proceeds to clean up the current proof
state in preparation for the reduction. This amounts to liberal applications of reduction, applying
function extensionality to avoid unsoundness where possible and eliminating type parameters of the
theorem. The latter is necessary because the monomorphizer will later attempt to generate specialized copies of
constant declarations. However, these copies may not use local type variables of a
theorem. In order to avoid this, Chako replaces the local type variables with global axioms of the
form `axiom a : Type`. Because these axioms are global constants, they can be used in any other
constant declarations as well. Later, in the translation step to Nunchaku's logic, these axioms are
emitted as uninterpreted types of the form `val a : type`.

After the cleanup step, Chako eliminates dependent types as described in @sect_trans_dtt. The
practical implementation takes three notable deviations from the theoretical description. First, it
supports arbitrary interleavings of type and term parameters to constants. Second, it treats some
logical connectives, such as Lean's `And`, `Iff`, `Eq`, and `Exists`, specially so they can later be directly
translated to Nunchaku's logic. This is not required for soundness but is useful because many of Nunchaku's backend solvers have
special support for these constants. Thus encoding them directly, instead of as inductive predicates,
improves the reasoning abilities of the solvers. Lastly, the encoding has special support for the previously mentioned
matchers and casesOn constructs. This special casing is necessary because they use a kind of dependently typed polymorphism that is
out of scope for the general translation. Fortunately, these constants are almost always used in a way
where they can be monomorphized easily on the fly. Hence, they are already eliminated in this
step, so the monomorphization does not have to deal with them.

The last step before producing the Nunchaku problem is the monomorphization. Like the dependent
type elimination, it is slightly generalized from its theoretical description in @sect_trans_poly.
The monomorphization also supports arbitrary interleavings of term and type parameters and, in
addition, any number of type parameters instead of just one. Handling more than one type parameter can be achieved
in two ways. Either each type argument can be tracked as an individual constraint variable, or the
type arguments of each constant are grouped and tracked as a vector-valued constraint variable.
These vector-valued variables have the advantage that they reduce problem size at the cost of a
more complex implementation. To observe the reduction in problem size, consider the map function on
lists:
```lean
def List.map {α β : Type} (f : α → β) (xs : List α) : List β :=
  match xs with
  | [] => []
  | x :: xs => f x :: map f xs
```
When `List.map` is called with `(Nat, String)` and `(String, Nat)` respectively, tracking each type
variable individually requires us to generate 4 copies of `List.map`: `(Nat, Nat)`, `(Nat, String)`,
`(String, Nat)`, and `(String, String)`. On the other hand, if they are tracked as vectors, we only need
to generate the actually necessary copies `(Nat, String)` and `(String, Nat)`. For this reason, Chako
implements the vector-valued approach.

The final step of Chako's pipeline is the handoff to Nunchaku. This step requires only minor
syntactic adjustments, such as translating (dependent) function types to either regular function
types or universal quantification, etc. Finally, Chako invokes Nunchaku on the generated problem and
uses its pipeline infrastructure to recover a counterexample if one is found.

#pagebreak(weak: true)

== Extending Nunchaku <sect_impl_nunchaku>
In addition to implementing Chako, I made several modifications to Nunchaku to improve the
performance for problems generated by Chako. These modifications amount to improving the
reduction pipeline, adding a new backend solver, and reporting or fixing several bugs in and around Nunchaku.

The changes to the pipeline are motivated by the prevalence of higher-order predicates in the
problems generated by Chako. These higher-order predicates occur due to Chako's invariant generator
introducing a higher-order parameter for each type argument of a polymorphic type. For example,
the invariant predicate for a polymorphic `List` type that got monomorphized on some type `a` looks
as follows:
```nun
pred [wf] List_inv : (a -> prop) -> List -> prop :=
  forall p . List_inv p Nil;
  forall p x xs . (p x && List_inv p xs) => List_inv p (Cons x xs).

val xs : List.
axiom List_inv a_inv xs.
```
By default, Nunchaku encodes these predicates into higher-order functions and then uses its encoding
for higher-order functions to deal with them. However, this encoding can make it difficult for
backend solvers to reliably solve the problems.

The primary mechanism of Nunchaku to avoid encoding higher-order functions is its specializer.
However, specialization occurs several steps before predicates are eliminated, making the
specializer oblivious to new opportunities further down the pipeline. To counteract this, I
modified the specializer to be able to work with the logics of later pipeline steps and placed a second
specialization pass after predicate elimination. This results in Nunchaku producing the same
function for `List_inv a_inv` as it would have for a manually specialized version:
```nun
pred [wf] List_inv' : List -> prop :=
  List_inv' Nil;
  forall x xs . (a_inv x && List_inv' xs) => List_inv' (Cons x xs).

# Both List_inv a_inv and List_inv' yield

rec List_inv : List -> prop :=
  List_inv ys =
    ys = Nil ||
    exists x xs. ys = Cons x xs && a_inv x && List_inv xs
```

On the solver side, I replaced the now deprecated CVC4 with its successor, cvc5 @cvc5. This change
is mostly a port of the CVC4 backend with two notable cvc5 specific modifications. The first
modification concerns the way in which Nunchaku queries how many values CVC4 synthesized
for an uninterpreted type such as `val a : type`. This functionality was fully removed from the textual interface of
cvc5. However, the cvc5 developers kindly implemented a replacement in the form of the, currently
undocumented, `get-model-domain-elements` upon my request.#footnote[https://github.com/cvc5/cvc5/pull/12088]

Just like the CVC4 backend, the cvc5 backend uses a portfolio approach where several instances of
the solver are run in parallel with different options. This is done because different configurations
might allow the solver to explore more approaches, thus increasing the chances of success. The second
modification concerns the options used for the portfolio. They were provided to me in
private communications by Andrew Reynolds and vary the strategy that cvc5 uses for reasoning
with quantifiers:
+ `--finite-model-find --fmf-mbqi=none --uf-ss-fair-monotone`
+ `--finite-model-find --decision=internal --uf-ss-fair-monotone`
+ `--finite-model-find --macros-quant --macros-quant-mode=all --uf-ss-fair-monotone`
+ `--finite-model-find --e-matching --uf-ss-fair-monotone`
+ `--finite-model-find --mbqi --fmf-mbqi=none`

Overall, I found the new cvc5 backend to outperform CVC4 by a few percent of wall-clock time on the Nunchaku test suite. In
addition, it solved 2 new problems on which CVC4 had given up previously. While this might not
sound like much, this small of a change can already have an impact for real-world use cases, as we
are going to see in @sect_case_studies_aa.

Lastly, during the development of Chako, I encountered several bugs along the entire reduction
pipeline of Nunchaku. In summary, I reported six bugs and fixed an additional four in
Nunchaku itself.#footnote[https://github.com/nunchaku-inria/nunchaku/issues?q=is%3Aissue%20state%3Aopen%20author%3AhargoniX]
While some of these bugs are only crashes within Nunchaku, at least three were silent soundness issues.
Furthermore, I encountered multiple crashes in SMBC due to violations of its main invariant
#footnote[https://github.com/c-cube/smbc/issues/7] and an unsoundness in cvc5 #footnote[https://github.com/cvc5/cvc5/issues/12208],
which is fixed by now. Even though these bugs can often be fixed quickly, they show that Nunchaku
has not yet matured to the point where one can expect it to just work. Getting to this point will
likely require pressure testing it more with a diverse set of problems.

#pagebreak(weak: true)

= Case Studies <sect_case_studies>
In this section, I present two case studies that demonstrate Chako’s capabilities when applied to
realistic verification problems: bisection on arrays and AA trees.

== Bisection <sect_case_studies_bisect>
The first example concerns the Lean standard library function `Array.binSearch`. It implements a
generic bisection algorithm for sorted arrays.
```lean
def binSearchAux {α : Type u} (lt : α → α → Bool) (as : Array α) (k : α)
    (lo : Fin (as.size + 1)) (hi : Fin as.size) (h : lo.1 ≤ hi.1) :
    Option α :=
  let m := (lo.1 + hi.1) / 2
  let a := as[m]
  if lt a k then
    if h' : m + 1 ≤ hi.1 then
      binSearchAux lt as k ⟨m + 1, by grind⟩ hi h'
    else
      none
  else if lt k a then
    if h' : m = 0 ∨ m - 1 < lo.1 then
      none
    else
      binSearchAux lt as k lo ⟨m - 1, by grind⟩ (by grind)
  else
    some a
termination_by hi.1 - lo.1

def binSearch {α : Type u} (as : Array α) (k : α) (lt : α → α → Bool) :
    Option α :=
  let lo := 0
  let hi := as.size - 1
  if h : lo < as.size then
    binSearchAux lt as k ⟨lo, by grind⟩ ⟨hi, by grind⟩ (by grind)
  else
    none
```
While the algorithmic side is quite simple, this program already suffices to demonstrate quite a few of the
features supported by Chako. First, the implementation is generic over the type of elements of the
array, calling the monomorphizer into action. Second, `binSearchAux` makes use of well-founded
recursion, carries a proof argument `h`, and works with the dependent type `Fin` to avoid
bounds checks.

Let us first consider a completeness theorem for `Array.binSearch`. The formulation uses the
proof-carrying type class `TotalOrder` to constrain the $<$ relation on `α`:
```lean
theorem complete [TotalOrder α] [DecidableLT α] (xs : Array α)
    (h : n ∈ xs) : xs.binSearch n (· < ·) = some n
```
Chako produces a counterexample in $0.2$ seconds, but it synthesizes a quite unreadable $<$
on `α`, which makes the counterexample difficult to interpret. Instead of checking this fully
general theorem, we can inspect a version specialized on `Nat` to get a more understandable output:
```lean
theorem complete (xs : Array Nat) (h : n ∈ xs) :
    xs.binSearch n (· < ·) = some n
```
For this version, Chako finds the counterexample `xs := #[1, 0]` and `n := 0`, which allows us to pinpoint the flaw in
this specification: `xs` is not restricted to sorted arrays. After adding this additional
assumption, Chako times out as expected.

In addition to completeness, we might also be interested in verifying soundness:
```lean
theorem sound (xs : Array Nat) :
    xs.binSearch n (· < ·) = some y ↔ n = y
```
Note that this theorem should hold even for unsorted arrays; if the function returns a value,
it should be the one we were looking for. However, the current statement does not capture this idea
correctly. Chako reveals the issue with the counterexample `n := 0`, `y := 0`, and `xs := #[]`.
The problem is that the theorem is stated too strongly, using `↔` instead of `→`. Once again, after
correcting the theorem statement, Chako times out.

== AA Trees <sect_case_studies_aa>
The second case study is an AA tree @aatrees formalization. AA trees are self-balancing search
trees designed with a focus on simple implementation. I selected this example for two reasons.
First, it involves a tree-shaped data structure and a non-trivial invariant, both things that are
not covered by the bisection example. Second, Nunchaku has a test called `slow_aa_trees`, derived from the Nitpick
case study on AA trees @nitpickpred, which heavily inspired this case study. This Nunchaku test case
remained unsolved (within a reasonable time) by Nunchaku until the improvements described in @sect_impl_nunchaku.

The foundation of this case study is the AA tree itself and some utility functions:

```lean
inductive AATree (α : Type u) where
  | nil
  | node (x : α) (level : Nat) (l r : AATree α)
```
#grid(
  columns: (1fr, 1fr),
```lean
def left : AATree α → AATree α
  | nil => .nil
  | node _ _ l _ => l
```,
```lean
def right : AATree α → AATree α
  | nil => .nil
  | node _ _ _ r => r
```
)

#grid(
  columns: (1fr, 1fr),
```lean
def lvl : AATree α → Nat
  | nil => 0
  | node _ lvl _ _ => lvl
```,
```lean
def isNil : AATree α → Bool
  | nil => true
  | node .. => false
```
)

```lean
def data (t : AATree α) (h : t.isNil = false := by grind) : α :=
  match t with
  | nil => False.elim (by grind [isNil])
  | node x .. => x

def mem (x : α) : AATree α → Prop
  | nil => False
  | node d _ l r => d = x ∨ mem x l ∨ mem x r
```

The self-balancing mechanism of AA trees revolves around maintaining the following invariant on the
level field of the nodes:
1. Nil nodes have level $0$
2. Leaf nodes have level $1$
3. A node's level must be $>=$ its right child's and $>$ than its left child's
4. Every node of level $> 1$ must have two non-nil children

This invariant can be almost directly expressed in Lean like so:
```lean
def WF : AATree α → Prop
  | nil => True -- nil has level 0 by definition
  | node _ lvl nil nil => lvl = 1
  | node _ lvl l r =>
    WF l ∧ WF r ∧
    lvl ≥ r.lvl ∧ lvl > l.lvl ∧ (lvl > 1 → (!l.isNil ∧ !r.isNil))
```
For performing the actual rebalancing operation, AA trees use two auxiliary functions called `skew` and `split`:
```lean
def skew : AATree α → AATree α
  | nil => nil
  | node x lvl l r =>
    if h : !l.isNil ∧ lvl = l.lvl then
      node l.data lvl l.left (node x lvl l.right r)
    else
      node x lvl l r

def split : AATree α → AATree α
  | nil => nil
  | node x lvl l r =>
    if h : !r.isNil ∧ lvl = r.right.lvl then
      node r.data (lvl + 1) (node x lvl l r.left) r.right
    else
      node x lvl l r
```
Both of these functions have some interesting properties with respect to the `mem` and `WF`
predicates:
#grid(
  columns: (1fr, 1fr),
  rows: (auto, auto),
  row-gutter: 2em,
  align: center,
```lean
mem x t ↔ t.skew.mem x
```,
```lean
mem x t ↔ t.split.mem x
```,
```lean
WF t → t.skew = t
```,
```lean
WF t → t.split = t
```
)
For the well-formedness properties, Chako (correctly) times out. For the membership ones,
Nunchaku's solvers report (also correctly) that they are generally true. Although Chako does not
provide a way to recover these proofs into Lean, this still provides the user reassurance during development.

With these auxiliary functions in place, the insertion operation can be defined almost exactly like
the one for primitive binary search trees:
```lean
def insert [LT α] [DecidableLT α] (t : AATree α) (x : α) : AATree α :=
  match t with
  | nil => .node x 1 nil nil
  | node y lvl l r =>
    let l' := if x < y then l.insert x else l
    let r' := if x > y then r.insert x else r
    split <| skew <| node y lvl l' r'
```
In order to keep the tree balanced, the `insert` function must preserve the `WF` invariant:
```lean
theorem WF_insert [TotalOrder α] [DecidableLT α] (t : AATree α) :
    WF t → WF (t.insert x)
```
Indeed Chako finds no counterexample for this within its default timeout.

Let us now introduce a suble bug into the implementation and observe how Chako fairs at detecting it:
```diff
 def skew : AATree α → AATree α
   | nil => nil
   | node x lvl l r =>
-    if h : !l.isNil ∧ lvl = l.lvl then
+    if h : !l.isNil ∧ lvl = r.lvl then
       node l.data lvl l.left (node x lvl l.right r)
     else
       node x lvl l r
```
As in the bisection example, Chako is configured to search for `AATree Nat` counterexamples to keep the
output readable. In this configuration, Chako quickly reports the counterexample `t := node 1 0 nil nil` and `x := 0`.
To confirm the bug, we can run the broken `insert` operation with the counterexample, which yields
`node 1 1 (node 0 1 nil nil) nil`. This tree violates condition 3 of the invariant, since the level of
the root node is not greater than its left child's.


#pagebreak(weak: true)

= Evaluation <sect_eval>
In this section I evaluate the behavior of Chako on a large set of problems derived from Lean's
standard library. Through this, I aim to answer three research questions:
- *RQ1*: Is the fragment handled by Chako powerful enough to encode frequently used constructs and
  types from Lean?
- *RQ2*: Is Chako able to reliably discover counterexamples for the fragment that it manages to
  encode?
- *RQ3*: How much does Chako's inherent unsoundness affect its false-positive rate in practice? In
  particular, can users trust that Chako produces correct counterexamples at least most of the time?

Because there is no standardized data set for counterexample finding in Lean available, this
experimental evaluation is based on a custom data set derived from Lean's standard library.
The data set contains #sound_num_total theorems from the standard library, collected from
the following built-in theories: `Array`, `BitVec`, `Fin`, `Int`, `List`, `Nat`, `Nat.Gcd`,
`Option`, and `TreeMap`.

To answer *RQ1* and *RQ3*, I run Chako on the already proven theorem statements.
Since all of these statements are known to be true, any counterexample reported by Chako must
necessarily be spurious. Moreover, each theorem statement is a well-typed Lean expression. Thus,
any errors encountered during encoding are most likely due to the target theorem lying outside the
fragment supported by Chako.

To answer *RQ2*, I follow the evaluation approach of Nitpick and previous Isabelle
counterexample finding tools: Taking correct theorem statements and mutating them in hopes of
creating false ones. Unlike in Isabelle, swapping out constants or removing assumptions can make a
Lean theorem no longer type-check due to dependent types. Because of this, I adapted a slightly
different approach to mutation. For each theorem I generate variants of the theorem with individual
assumptions removed, logical and relational connectives swapped with others, and variables replaced
with other variables of the same type. Since any of these mutations may render the statement ill-typed,
each candidate mutant is type-checked and included in the benchmark suite only if it is well-typed.
Using this process, I derived a total of #perf_num_total mutants from the original dataset.

Note that this mutation procedure does not exclusively produce false statements due to symmetries or
inconsistent assumptions in the original theorems.  For example, the theorem `Nat.mul_ne_zero` with
the original statement $n != 0 -> m != 0 -> n dot m != 0$ can be mutated to a statement such as
$n != 0 -> m != 0 -> n dot n != 0$, which is still true. Despite this, the vast majority of mutants
are going to be false and thus amenable to counterexample search.

Both of these experiments were run on a 13th Gen Intel(R) Core(TM) i7-1360P CPU with 32 GB of RAM.
I used Lean #link("https://github.com/leanprover/lean4/releases/tag/v4.24.1", [4.24.1]),
Chako at commit
#link("https://github.com/hargoniX/chako-lean/commit/e2c8eeff6c359061caf185f911d233635270e99b", [e2c8eef]), Nunchaku at commit
#link("https://github.com/nunchaku-inria/nunchaku/commit/fc0a916451eae2c333ccbcd9d6716418bb2f4fb0", [fc0a916])
with the backend solvers cvc5 at commit #link("https://github.com/cvc5/cvc5/commit/724682fa53aae3d870377065bbe3bed37ae9697c", [724682fa5]), Kodkod from Isabelle
2025, and SMBC at commit #link("https://github.com/c-cube/smbc/commit/930278367b0a4a46eb0378455fe78dc99fc3133e", [9302783]). All problems were run with Chako's
default wall time limit of ten seconds, one-by-one in sequence with access to all cores.

#figure(
  sound_table,
  caption: [Results of Chako on Correct Statements]
) <sound_table>

The results of running Chako on the set of correct statements are shown in @sound_table. Each row
reports, for the corresponding theory, the percentage of outcomes together with the total number of
problems in that theory. The final row presents the overall result percentages and the total number
of theorems.

As we can see, Chako only found counterexamples (*SAT*) for $0.4%$ of the theorems. After
manual inspection, all of these counterexamples can be attributed to an unsoundness in the reduction
to Kodkod (TODO: footnote with issue). Furthermore, Chako managed to prove (*UNSAT*)
$28.1%$ of the statements and gave up on $44.8%$ (*Unknown*). It was particularly successful
at proving theorems in the `Option` theory, likely because most of its theorems can be proven by case distinction.
Overall the low false positive rate and the fact that the solvers identify many theorems as true
indicate that the theoretical unsoundness of Chako does not seem to manifest frequently in practice.

Furthermore, with the exception of `Array` and `TreeMap`, Chako achieves translation success rates
of above $95%$ when translating theorems to Nunchaku's input language. The high error rates on `Array` and `TreeMap`
stem from their use of unsupported polymorphism variants. `Array` defines many higher-order functions
in terms of their monadic counterpart. For example, `Array.map` is defined as `Array.mapM`,
instantiated with the `Id` monad. Monadic functions such as `Array.mapM` abstract over the monad
they run in as a type constructor `m : Type → Type`. Because Chako does not know how to handle this
kind of polymorphism, it gives up on all theorems that use these functions. `TreeMap` on the other
hand is based on type the type `DTreeMap` of dependent tree maps. In these kinds of maps the type of
values is a generic type that may depend on the key type `(value : key → Type)`, this type of
polymorphism is also unsupported. Thus Chako has to give up an any theorem containing a `TreeMap`.
This demonstrates that there is currently a tension between using Lean's powerful
polymorphism for building abstract definitions and using Chako for finding counterexamples in
simpler structures derived from such abstract definitions.

#figure(
  perf_table,
  caption: [Results of Chako on Mutated Statements]
) <perf_table>

The results of the mutation experiment can be seen in @perf_table. The structure of this table
mimicks that of @sound_table. Just like in the first experiment, `Array` and `TreeMap` have
exceptionally high error rates due to their use of unsupported polymorphism while almost all other
problems can be translated. Including these translation errors, Chako manages to find a
counterexample for $60.2%$ of the mutants. When excluding the translation errors, we can obtain the
success rate of Nunchaku on the problems it was able to run on which amounts of $81.4%$. This shows
that if the mutant can be translated to Nunchaku, Chako is has good chances at finding a counterexample
if one exists.

As explained in the setup of the mutation experiment, not all of the generated mutants are
necessarily false. This phenomenon is the reason for the $3.5%$ of mutants proven correct overall.
Note that the $10.3%$ of mutants that the solvers gave up on likely contain both true and false
statements because of this.

Overall both of these experiments demonstrate that Chako is sound in practice, able to encode large
parts of Lean's standard library, and also able to find counterexamples for it quite reliably.

The main potential source of error in this evaluation is the generation of the wrong statements.
It is unclear whether the mutation procedure does generate the kinds of statements that users are
practically interested in finding counterexamples for. Furthermore, the ability of Chako to find counterexamples
can vary a lot depending on the kinds of problems. This can be observed in the comparatively low
success rate of $57.1%$ on `Nat.Gcd`. For both of these reasons, the ability of Chako to discover
counterexamples in practice may deviate from the numbers presented here.

#pagebreak(weak: true)
= Conclusion and Future Work <sect_conclusion>
In this thesis I presented Chako, the first finite model finding tool integrated with a dependently
typed theorem prover. While the underlying translation is limited to only ML-style polymorphism for
now, Chako is already able to translate and find counterexamples for a large part of the Lean
standard library.

With the basic theoretical approach in place, a natural next step would be to extend the support for
polymorphism.  The most frequently used advanced kinds of polymorphism in Lean are higher-kinded
types and polymorphism over dependent types. As the evaluation has shown, even innocuous-looking
functions can make use of these advanced kinds of polymorphism under the hood. Thus, in order to
extend the coverage of Chako, implementing support for at least both of these is crucial.

Furthermore, as already mentioned in @sect_trans_dtt, the dependent type elimination is done before
monomorphization purely for simplicity reasons. It is possible that switching these translation
passes around might yield a better translation. In particular, it would resolve the current issues
with function extensionality and simplify the generation of the inductive invariants for dependent
types. However, this requires developing a monomorphization technique that works in the presence of
dependent types.

On the solver side, many additional improvements to Nunchaku are possible. As I have shown, it
currently suffers from inconsistencies on a somewhat regular basis. One way to prevent this in
the future would be to test it with a large and diverse set of problems, in the hope of discovering
these inconsistencies before they affect users. In addition, the authors of Nunchaku and I
identified several opportunities for generating better problems for the backend solvers while
inspecting the problems generated by Chako. For example, standard compiler optimizations, such as
inlining, appear promising for further simplifying the generated problems. Together, these
approaches could improve both the soundness and success rate of Nunchaku.

Finally, the current evaluation relies on an artificial data set that does not necessarily
correspond to the kinds of problems that Chako will encounter in the real world. While proof
automation tools in Lean can be evaluated using the standard library and other community-maintained
libraries, there is currently no real-world benchmark of false statements for counterexample
finding in Lean. Ideally a data set analogous to SMT-LIB @smtlib, containing both true and false
statements drawn from a wide range of domains, could be developed for Lean. Such a data set would
enable both proof automation and counterexample-finding tools to be evaluated on the same realistic
problems. This would improve both our understanding of their strengths and limitations as well as
the comparability between them.

#v(15pt)
#[Total characters: #total-characters] <no-wc>
